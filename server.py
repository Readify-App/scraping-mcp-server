# server.py
# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨MCPã‚µãƒ¼ãƒãƒ¼

import logging
import asyncio
import json
import os
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse
from typing import Any, Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from bs4.element import Tag
from mcp.server.fastmcp import FastMCP
import aiohttp
from aiohttp import ClientTimeout, BasicAuth

# Playwrightã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    async_playwright = None
    PLAYWRIGHT_AVAILABLE = False

# Google Sheets APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from google.oauth2 import service_account
    from googleapiclient.discovery import build
    from googleapiclient.errors import HttpError
    GOOGLE_SHEETS_AVAILABLE = True
except ImportError:
    service_account = None
    build = None
    HttpError = None
    GOOGLE_SHEETS_AVAILABLE = False

# ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å‹•çš„ã«æ±ºå®šï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ï¼‰
_log_dir = Path(__file__).parent
_log_file = _log_dir / 'debug.log'
# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
_log_dir.mkdir(parents=True, exist_ok=True)

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(str(_log_file)),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
MAX_BROWSERS = 5
browser_semaphore = asyncio.Semaphore(MAX_BROWSERS)

CLOUD_GYM_BASE_URL = "https://cloud-gym.com/personal-fitness"
CLOUD_GYM_POST_TYPE = "introduce"
CLOUD_GYM_API_ENDPOINT = f"{CLOUD_GYM_BASE_URL.rstrip('/')}/wp-json/wp/v2/{CLOUD_GYM_POST_TYPE}"
CLOUD_GYM_DEFAULT_FIELDS = "id,title,excerpt,date,link,slug"

# Rakuraku Media School settings
RAKURAKU_SITE_URL = "https://rakuraku.app/media/school"
RAKURAKU_POST_TYPE = "school-list"
RAKURAKU_API_BASE = f"{RAKURAKU_SITE_URL.rstrip('/')}/wp-json/wp/v2"
RAKURAKU_DEFAULT_FIELDS = "id,title,slug,date,link,status"
RAKURAKU_WP_USERNAME = "rakuraku-admin-school"
RAKURAKU_WP_APP_PASSWORD = "ajBN QdvS fPGS 0L9O SkeV CgVJ"
RAKURAKU_FIELD_CONTAINERS = {"custom_fields", "meta", "acf"}
RAKURAKU_ALLOWED_STATUSES = ["publish", "draft"]


def _rakuraku_credentials_ready() -> bool:
    return bool(RAKURAKU_WP_USERNAME and RAKURAKU_WP_APP_PASSWORD)


def _rakuraku_missing_credentials_message() -> str:
    return (
        "Rakuraku Media School ã®WordPress APIã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã«ã¯ã€"
        "ç’°å¢ƒå¤‰æ•° RAKURAKU_WP_USERNAME ã¨ RAKURAKU_WP_APP_PASSWORD ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
    )


def _wp_extract_text(field: Any) -> str:
    if isinstance(field, dict):
        return field.get("rendered") or field.get("raw") or ""
    if isinstance(field, list):
        return ", ".join(str(item) for item in field if item not in (None, ""))
    if field is None:
        return ""
    return str(field)


def _flatten_field_value(value: Any) -> Any:
    if isinstance(value, list):
        if len(value) == 1:
            return _flatten_field_value(value[0])
        return [_flatten_field_value(v) for v in value]
    if isinstance(value, dict):
        return {k: _flatten_field_value(v) for k, v in value.items()}
    return value


def _rakuraku_collect_custom_fields(post: Dict[str, Any]) -> Dict[str, Any]:
    fields: Dict[str, Any] = {}
    for key in ("custom_fields", "meta", "acf"):
        raw = post.get(key)
        if isinstance(raw, dict):
            for f_key, f_val in raw.items():
                fields[f_key] = _flatten_field_value(f_val)
    return fields


def _truncate_value(value: Any, limit: int = 120) -> str:
    text = _wp_extract_text(value) if isinstance(value, dict) else str(value)
    text = text.strip()
    if len(text) > limit:
        return text[: limit - 3] + "..."
    return text


def _strip_html(html: str) -> str:
    try:
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text(separator=" ", strip=True)
    except Exception:
        return html


def _rakuraku_format_summary(post: Dict[str, Any], include_fields: bool = False) -> str:
    title = _wp_extract_text(post.get("title"))
    link = post.get("link", "")
    post_id = post.get("id")
    slug = post.get("slug", "")
    status = post.get("status", "")
    date_str = post.get("date", "")
    
    summary = [
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        f"ğŸ“ {title or 'ã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®š'}",
        f"ğŸ†” ID: {post_id} / slug: {slug}",
        f"ğŸ“… å…¬é–‹æ—¥: {date_str or 'N/A'} / status: {status or 'ä¸æ˜'}",
        f"ğŸ”— {link or 'ãƒªãƒ³ã‚¯ãªã—'}",
    ]
    
    if include_fields:
        fields = _rakuraku_collect_custom_fields(post)
        if fields:
            preview_items = []
            for key, value in list(fields.items())[:6]:
                preview_items.append(f"{key}={_truncate_value(value, 60)}")
            if preview_items:
                summary.append("ğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:")
                summary.append("   " + ", ".join(preview_items))
    
    return "\n".join(summary)


def _rakuraku_format_detail(post: Dict[str, Any]) -> str:
    title = _wp_extract_text(post.get("title"))
    summary = [
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
        f"ğŸ“ {title or 'ã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®š'}",
        f"ğŸ†” ID: {post.get('id')} / slug: {post.get('slug')}",
        f"ğŸ“… å…¬é–‹æ—¥: {post.get('date')} / æœ€çµ‚æ›´æ–°: {post.get('modified')}",
        f"ğŸ‘¤ author: {post.get('author')}",
        f"ğŸ”— {post.get('link')}",
        "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
    ]
    
    content = post.get("content", {}).get("rendered")
    if content:
        stripped = _strip_html(content)
        summary.append("ğŸ“ æœ¬æ–‡ï¼ˆå†’é ­200æ–‡å­—ï¼‰:")
        summary.append(_truncate_value(stripped, 200))
    
    excerpt = post.get("excerpt", {}).get("rendered")
    if excerpt:
        summary.append("\nğŸ’¡ æŠœç²‹:")
        summary.append(_truncate_value(_strip_html(excerpt), 160))
    
    fields = _rakuraku_collect_custom_fields(post)
    if fields:
        summary.append("\nğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ä¸€è¦§:")
        for key, value in fields.items():
            summary.append(f"- {key}: {_truncate_value(value)}")
    else:
        summary.append("\nğŸ”§ ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    
    return "\n".join(summary)


async def _rakuraku_wp_get(path: str, params: Optional[Dict[str, Any]] = None) -> Tuple[Any, Dict[str, str]]:
    if not _rakuraku_credentials_ready():
        raise RuntimeError(_rakuraku_missing_credentials_message())
    
    url = path
    if not url.startswith("http"):
        url = f"{RAKURAKU_API_BASE.rstrip('/')}/{path.lstrip('/')}"
    
    auth = BasicAuth(RAKURAKU_WP_USERNAME, RAKURAKU_WP_APP_PASSWORD)
    timeout = ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.get(url, params=params, auth=auth) as response:
            try:
                payload = await response.json(content_type=None)
            except Exception:
                payload = await response.text()
            
            if response.status >= 400:
                error_details = payload if isinstance(payload, dict) else {"message": str(payload)}
                raise RuntimeError(
                    f"WordPress APIã‚¨ãƒ©ãƒ¼ (HTTP {response.status}): {json.dumps(error_details, ensure_ascii=False)}"
                )
            
            headers = {k: v for k, v in response.headers.items()}
            return payload, headers


async def _rakuraku_find_post(identifier: str) -> Optional[Dict[str, Any]]:
    params = {"context": "edit", "status": ",".join(RAKURAKU_ALLOWED_STATUSES)}
    
    if identifier.isdigit():
        path = f"{RAKURAKU_POST_TYPE}/{identifier}"
        post, _ = await _rakuraku_wp_get(path, params=params)
        if isinstance(post, dict):
            return post
        return None
    
    # slug or title search
    slug_params = params | {"slug": identifier, "per_page": 1}
    posts, _ = await _rakuraku_wp_get(RAKURAKU_POST_TYPE, params=slug_params)
    if isinstance(posts, list) and posts:
        return posts[0]
    
    search_params = params | {"search": identifier, "per_page": 1}
    posts, _ = await _rakuraku_wp_get(RAKURAKU_POST_TYPE, params=search_params)
    if isinstance(posts, list) and posts:
        return posts[0]
    
    return None


async def _rakuraku_wp_post(path: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    if not _rakuraku_credentials_ready():
        raise RuntimeError(_rakuraku_missing_credentials_message())
    
    url = path
    if not url.startswith("http"):
        url = f"{RAKURAKU_API_BASE.rstrip('/')}/{path.lstrip('/')}"
    
    auth = BasicAuth(RAKURAKU_WP_USERNAME, RAKURAKU_WP_APP_PASSWORD)
    timeout = ClientTimeout(total=30)
    
    async with aiohttp.ClientSession(timeout=timeout) as session:
        async with session.post(url, json=payload, auth=auth) as response:
            try:
                payload_resp = await response.json(content_type=None)
            except Exception:
                payload_resp = await response.text()
            
            if response.status >= 400:
                error_details = payload_resp if isinstance(payload_resp, dict) else {"message": str(payload_resp)}
                raise RuntimeError(
                    f"WordPress APIã‚¨ãƒ©ãƒ¼ (HTTP {response.status}): {json.dumps(error_details, ensure_ascii=False)}"
                )
            if isinstance(payload_resp, dict):
                return payload_resp
            raise RuntimeError("äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§ã™ã€‚JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å—ä¿¡ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")


def _rakuraku_format_update_summary(
    post: Dict[str, Any],
    updated_fields: Dict[str, Any],
    field_group: str
) -> str:
    title = _wp_extract_text(post.get("title"))
    lines = [
        "âœ… æ›´æ–°æˆåŠŸ",
        f"ID: {post.get('id')} / slug: {post.get('slug')}",
        f"ã‚¿ã‚¤ãƒˆãƒ«: {title or '(ã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®š)'}",
        f"å¯¾è±¡: {field_group}",
        "",
        "æ›´æ–°å†…å®¹:"
    ]
    for key, value in updated_fields.items():
        lines.append(f"  â€¢ {key}: {value}")
    return "\n".join(lines)


def _rakuraku_build_status_param(arg: Optional[str]) -> str:
    tokens: List[str] = []
    if arg:
        tokens = [token.strip().lower() for token in arg.split(",") if token.strip()]
    selected = [token for token in tokens if token in RAKURAKU_ALLOWED_STATUSES]
    if not selected:
        selected = RAKURAKU_ALLOWED_STATUSES.copy()
    ordered_unique: List[str] = []
    for status in selected:
        if status not in ordered_unique:
            ordered_unique.append(status)
    return ",".join(ordered_unique)


def _rakuraku_normalize_single_status(status: Optional[str]) -> str:
    value = (status or "").strip().lower()
    if value in RAKURAKU_ALLOWED_STATUSES:
        return value
    return "draft"


def _rakuraku_parse_fields_json(raw: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    if not raw or not raw.strip():
        return None, None
    try:
        data = json.loads(raw)
    except json.JSONDecodeError as exc:
        return None, f"âŒ JSONã®å½¢å¼ãŒæ­£ã—ãã‚ã‚Šã¾ã›ã‚“: {exc}"
    if not isinstance(data, dict):
        return None, "âŒ JSONã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆKey/Valueå½¢å¼ï¼‰ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    return data, None


def _rakuraku_build_edit_url(post_id: Any) -> str:
    try:
        pid = int(post_id)
    except Exception:
        pid = post_id
    return f"{RAKURAKU_SITE_URL.rstrip('/')}/wp-admin/post.php?post={pid}&action=edit"


def _rakuraku_format_post_action_result(action: str, post: Dict[str, Any]) -> str:
    title = _wp_extract_text(post.get("title")) or "(ã‚¿ã‚¤ãƒˆãƒ«æœªè¨­å®š)"
    status = post.get("status", "unknown")
    post_id = post.get("id")
    link = post.get("link") or ""
    edit_url = _rakuraku_build_edit_url(post_id) if post_id else "N/A"
    lines = [
        action,
        f"ğŸ†” ID: {post_id} / status: {status}",
        f"ğŸ“ ã‚¿ã‚¤ãƒˆãƒ«: {title}",
        f"ğŸ”— è¡¨ç¤ºURL: {link or 'N/A'}",
        f"âœï¸ ç·¨é›†URL: {edit_url}",
    ]
    return "\n".join(lines)


async def _rakuraku_handle_update_tool(
    *,
    post_type: str,
    post_id: int,
    fields_json: str,
    container: str,
    wrap_payload: bool,
    label: str
) -> str:
    try:
        data = json.loads(fields_json)
    except json.JSONDecodeError as exc:
        return (
            "âŒ JSONã®å½¢å¼ã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚\n"
            f"ã‚¨ãƒ©ãƒ¼: {exc}\n"
            "ä¾‹: {\"ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å\": \"å€¤\"}"
        )
    
    if not isinstance(data, dict) or not data:
        return "âŒ JSONã¯ã‚­ãƒ¼ã¨å€¤ã‚’æŒã¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆå½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    
    container = (container or "custom_fields").strip()
    wrap_payload = bool(wrap_payload)
    
    if wrap_payload:
        if container not in RAKURAKU_FIELD_CONTAINERS:
            return (
                f"âŒ container='{container}' ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"
                " ä½¿ç”¨å¯èƒ½: custom_fields / meta / acf"
            )
        payload = {container: data}
        summary_fields = data
        field_group = f"{label}:{container}"
    else:
        payload = data
        summary_fields = data
        field_group = f"{label}:raw"
    
    logger.info(
        "[Rakuraku] æ›´æ–°é–‹å§‹ post_type=%s id=%s container=%s wrap=%s",
        post_type,
        post_id,
        container,
        wrap_payload,
    )
    
    try:
        post = await _rakuraku_wp_post(f"{post_type}/{post_id}", payload)
    except RuntimeError as exc:
        logger.error(
            "[Rakuraku] æ›´æ–°å¤±æ•— post_type=%s id=%s : %s",
            post_type,
            post_id,
            exc,
        )
        return f"âŒ æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{exc}"
    
    return _rakuraku_format_update_summary(post, summary_fields, field_group)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# MCPã‚µãƒ¼ãƒãƒ¼ä½œæˆ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
mcp = FastMCP("web-scraping-server")

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ„ãƒ¼ãƒ«å®šç¾©
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@mcp.tool()
async def fetch_page_content(url: str) -> str:
    """
    æŒ‡å®šURLã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    é€šå¸¸ã®HTMLãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URLï¼ˆä¾‹ï¼šã€Œhttps://example.com/pageã€ï¼‰
    
    Returns:
        ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆJSONå½¢å¼ï¼‰
    """
    logger.info(f"fetch_page_content called with url={url}")
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=ClientTimeout(total=20)) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch page: {response.status}"
                    logger.error(error_msg)
                    return f"ã‚¨ãƒ©ãƒ¼: {error_msg}\nURL: {url}"
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
                for tag in soup(["script", "style", "noscript", "header", "footer", "nav"]):
                    tag.decompose()
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
                main_content = None
                
                # 1. main ã‚¿ã‚°
                main_content = soup.find("main")
                
                # 2. article ã‚¿ã‚°
                if not main_content:
                    main_content = soup.find("article")
                
                # 3. role="main" å±æ€§
                if not main_content:
                    main_content = soup.find(attrs={"role": "main"})
                
                # 4. id or class ã« content/main ãŒå«ã¾ã‚Œã‚‹è¦ç´ 
                if not main_content:
                    for selector in ["#content", ".content", "#main", ".main", 
                                   "[id*='content']", "[class*='content']"]:
                        main_content = soup.select_one(selector)
                        if main_content:
                            break
                
                # 5. bodyå…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not main_content:
                    main_content = soup.find("body")
                
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                if main_content:
                    # æ”¹è¡Œã‚„ã‚¿ãƒ–ã‚’æ­£è¦åŒ–
                    text = main_content.get_text(separator="\n", strip=True)
                    # é€£ç¶šã™ã‚‹ç©ºç™½è¡Œã‚’å‰Šé™¤
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    content = '\n'.join(lines)
                else:
                    content = ""
                
                # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å–å¾—ï¼ˆå‚è€ƒæƒ…å ±ã¨ã—ã¦ï¼‰
                title = ""
                title_tag = soup.find("title")
                if title_tag:
                    title = title_tag.get_text(strip=True)
                
                logger.info(f"Successfully extracted content: {len(content)} chars")
                
                # çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™
                result = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                result += f"ğŸ“„ {title}\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                result += f"ğŸ”— URL: {url}\n"
                result += f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(content)} æ–‡å­—\n\n"
                result += f"ã€æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘\n\n"
                result += content
                
                return result
                
    except Exception as e:
        logger.exception(f"Error in fetch_page_content: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nURL: {url}"


@mcp.tool()
async def fetch_page_content_with_playwright(url: str) -> str:
    """
    Playwrightã‚’ä½¿ç”¨ã—ã¦JavaScript/SPA/Reactã‚µã‚¤ãƒˆã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¾ã™ã€‚
    å‹•çš„ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    Shadow DOMã‚„ã‚«ã‚¹ã‚¿ãƒ è¦ç´ ã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URLï¼ˆä¾‹ï¼šã€Œhttps://example.com/spa-pageã€ï¼‰
    
    Returns:
        ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆJSONå½¢å¼ï¼‰
    """
    logger.info(f"fetch_page_content_with_playwright called with url={url}")
    
    if not PLAYWRIGHT_AVAILABLE:
        error_msg = "Playwright is not available. Please install it with: pip install playwright && playwright install"
        logger.error(error_msg)
        return f"ã‚¨ãƒ©ãƒ¼: {error_msg}"
    
    if not async_playwright:
        return "ã‚¨ãƒ©ãƒ¼: Playwright not available"
    
    # PDFã‚’äº‹å‰ã«é™¤å¤–
    if url.lower().endswith('.pdf'):
        logger.warning(f"Skipping PDF: {url}")
        return f"ã‚¨ãƒ©ãƒ¼: PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“\nURL: {url}"
    
    async with browser_semaphore:  # ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
        browser = None
        context = None
        page = None
        try:
            logger.debug(f"Starting Playwright extraction for: {url}")
            async with async_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--single-process']
                )
                # Contextã‚’ä½œæˆ
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                # User-Agentã‚’è¨­å®š
                await page.set_extra_http_headers({
                    "User-Agent": (
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    ),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                })
                
                # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                logger.debug(f"Navigating to {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=15000)
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè½ã¡ç€ãã¾ã§å¾…æ©Ÿ
                await page.wait_for_load_state("networkidle")
                
                # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼åŒæ„ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®å‡¦ç†
                try:
                    consent_selectors = [
                        "#accept",
                        ".accept.uc-accept-button",
                        "[data-action='consent'][data-action-type='accept']",
                        "button:has-text('ã™ã¹ã¦å—ã‘å…¥ã‚Œã‚‹')",
                        "button:has-text('åŒæ„')",
                        "button:has-text('Accept')",
                        "button:has-text('OK')",
                    ]
                    
                    for selector in consent_selectors:
                        try:
                            if await page.locator(selector).count() > 0:
                                await page.click(selector, timeout=2000)
                                await page.wait_for_timeout(1000)
                                break
                        except:
                            continue
                except:
                    pass
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã‚’å¾…ã¤
                try:
                    await page.wait_for_selector(
                        "main, article, [role='main'], .content, #content, .main-content",
                        timeout=10000
                    )
                except:
                    logger.warning("No main content selector found, continuing...")
                
                # SPAã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…ã¤
                await page.wait_for_timeout(5000)
                
                # JavaScriptã§ç›´æ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆShadow DOMå¯¾å¿œï¼‰
                content_data = await page.evaluate("""
                    () => {
                        // Shadow DOMã‚‚å«ã‚ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹é–¢æ•°
                        function extractAllText(element) {
                            let text = '';
                            
                            // Shadow rootãŒã‚ã‚‹å ´åˆ
                            if (element.shadowRoot) {
                                const shadowElements = element.shadowRoot.querySelectorAll('*');
                                shadowElements.forEach(el => {
                                    text += extractAllText(el) + ' ';
                                });
                            }
                            
                            // é€šå¸¸ã®DOMè¦ç´ ã®å‡¦ç†
                            if (element.nodeType === Node.TEXT_NODE) {
                                text += element.textContent || '';
                            } else if (element.nodeType === Node.ELEMENT_NODE) {
                                // ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–
                                if (!['SCRIPT', 'STYLE', 'NOSCRIPT', 'META', 'LINK'].includes(element.tagName)) {
                                    for (const child of element.childNodes) {
                                        text += extractAllText(child) + ' ';
                                    }
                                }
                            }
                            
                            return text;
                        }
                        
                        // äº‹å‰ã«scriptã¨styleã‚’å…¨ã¦å‰Šé™¤
                        document.querySelectorAll('script, style, noscript').forEach(el => el.remove());
                        
                        // ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
                        const selectors = [
                            'main',
                            'article',
                            '[role="main"]',
                            '#content',
                            '.content',
                            '#main',
                            '.main',
                            '.main-content',
                            '.page-content',
                            'body'
                        ];
                        
                        let mainContent = null;
                        for (const selector of selectors) {
                            mainContent = document.querySelector(selector);
                            if (mainContent) break;
                        }
                        
                        // ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
                        if (mainContent) {
                            const excludeSelectors = ['header', 'footer', 'nav', '.header', '.footer', '.navigation'];
                            excludeSelectors.forEach(selector => {
                                const elements = mainContent.querySelectorAll(selector);
                                elements.forEach(el => el.remove());
                            });
                            
                            mainContent.querySelectorAll('script, style, noscript').forEach(el => el.remove());
                        }
                        
                        // Shadow DOMã‚‚å«ã‚ãŸå…¨ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                        const shadowDomText = mainContent ? extractAllText(mainContent) : '';
                        const innerText = mainContent ? mainContent.innerText : '';
                        const textContent = mainContent ? mainContent.textContent : '';
                        
                        // ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å–å¾—
                        const title = document.title || '';
                        
                        return {
                            title: title,
                            shadowDomText: shadowDomText.trim(),
                            innerText: innerText.trim(),
                            textContent: textContent.trim(),
                            shadowDomLength: shadowDomText.length,
                            innerTextLength: innerText.length,
                            textContentLength: textContent.length
                        };
                    }
                """)
                
                # mailto: ã¨ tel: ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                try:
                    link_hrefs = await page.evaluate("""
                        () => Array.from(document.querySelectorAll('a[href^="mailto:"], a[href^="tel:"]'))
                            .map(a => a.getAttribute('href') || '')
                    """)
                except Exception:
                    link_hrefs = []
                
                # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é¸æŠ
                content = ""
                
                # Nuxt.js/Vue.jsã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                def is_json_data(text):
                    if not text:
                        return False
                    json_patterns = ['window.__NUXT__', '["[\"Reactive\"', '{"data":', 'googleapis.com']
                    return any(pattern in text[:500] for pattern in json_patterns)
                
                if content_data.get('innerText') and len(content_data['innerText']) > 100 and not is_json_data(content_data['innerText']):
                    content = content_data['innerText']
                    logger.debug(f"Using innerText: {len(content)} chars")
                elif content_data.get('shadowDomText') and len(content_data['shadowDomText']) > 100 and not is_json_data(content_data['shadowDomText']):
                    content = content_data['shadowDomText']
                    logger.debug(f"Using shadowDomText: {len(content)} chars")
                elif content_data.get('textContent') and not is_json_data(content_data['textContent']):
                    content = content_data['textContent']
                    logger.debug(f"Using textContent: {len(content)} chars")
                else:
                    content = ""
                    logger.warning("No valid content found (JSON data detected)")
                
                # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
                if content:
                    content = ' '.join(content.split())
                    lines = content.split('.')
                    content = '.\n'.join(line.strip() for line in lines if line.strip())
                
                # æŠ½å‡ºã—ãŸãƒ¡ãƒ¼ãƒ«ãƒ»é›»è©±ç•ªå·ã‚’æ•´å½¢
                extracted_emails = []
                extracted_phones = []
                try:
                    for href in link_hrefs:
                        h = (href or '').strip()
                        if h.lower().startswith('mailto:'):
                            email = h.split(':', 1)[1].split('?', 1)[0]
                            if email:
                                extracted_emails.append(email)
                        elif h.lower().startswith('tel:'):
                            number = h.split(':', 1)[1]
                            if number:
                                extracted_phones.append(number)
                except Exception:
                    pass
                
                logger.info(f"Successfully extracted content: {len(content)} chars")
                
                # çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™
                result = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                result += f"ğŸ“„ {content_data.get('title', '')}\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                result += f"ğŸ”— URL: {url}\n"
                result += f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(content)} æ–‡å­—\n"
                result += f"ğŸ”§ æŠ½å‡ºæ–¹æ³•: Playwright (JavaScript rendering)\n"
                
                if extracted_emails:
                    result += f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {', '.join(set(extracted_emails))}\n"
                if extracted_phones:
                    result += f"ğŸ“ é›»è©±ç•ªå·: {', '.join(set(extracted_phones))}\n"
                
                result += f"\nã€æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘\n\n"
                result += content
                
                if len(content) < 100:
                    result += "\n\nâš ï¸ è­¦å‘Š: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚èªè¨¼ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã‹ã€å‹•çš„èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãƒšãƒ¼ã‚¸ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
                
                return result
                
        except Exception as e:
            logger.exception(f"Error in fetch_page_content_with_playwright: {e}")
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nURL: {url}"
        finally:
            # å®Œå…¨ãªçµ‚äº†å‡¦ç†
            try:
                if page:
                    await page.close()
            except:
                pass
            try:
                if context:
                    await context.close()
            except:
                pass
            try:
                if browser:
                    await browser.close()
                    await asyncio.sleep(1)
            except:
                pass


@mcp.tool()
async def extract_site_links(url: str) -> str:
    """
    å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰header/footer/navã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã€ä»®æƒ³ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚
    é€šå¸¸ã®HTMLãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å¯¾è±¡ã‚µã‚¤ãƒˆã®URLï¼ˆä¾‹ï¼šã€Œhttps://www.goldsgym.jpã€ï¼‰
    
    Returns:
        ã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯æƒ…å ±ã®JSONæ–‡å­—åˆ—ã€‚å„ãƒªãƒ³ã‚¯ã«ã¯text, urlã€content_headings(è¦‹å‡ºã—ãƒªã‚¹ãƒˆ)ãŒå«ã¾ã‚Œã‚‹
    """
    logger.info(f"extract_site_links called with url={url}")
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=ClientTimeout(total=20)) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch page: {response.status}"
                    logger.error(error_msg)
                    return json.dumps({
                        "error": error_msg,
                        "base_url": url,
                        "links": []
                    }, ensure_ascii=False)
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
                for tag in soup(["script", "style", "noscript"]):
                    tag.decompose()
                
                # header/footer/navè¦ç´ ã‚’æ¢ã™
                header = soup.find("header") or soup.select_one('[role="banner"]')
                footer = soup.find("footer") or soup.select_one('[role="contentinfo"]')
                
                # ç‹¬ç«‹ã—ãŸnavè¦ç´ ã‚’æ¢ã™ï¼ˆheaderã®å¤–ã«ã‚ã‚‹ã‚‚ã®ï¼‰
                nav_elements = soup.find_all("nav")
                independent_navs = []
                if header and isinstance(header, Tag):
                    header_navs = header.find_all("nav")
                    independent_navs = [nav for nav in nav_elements if nav not in header_navs]
                else:
                    independent_navs = nav_elements
                
                # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹å†…éƒ¨é–¢æ•°
                def extract_links_from_element(element):
                    if element is None:
                        return []
                    
                    links = []
                    for link in element.find_all('a', href=True):
                        text = link.get_text(strip=True)
                        href = link['href']
                        
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        absolute_url = urljoin(url, href)
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿å‡¦ç†
                        if text:
                            links.append({
                                'text': text,
                                'url': absolute_url
                            })
                    return links
                
                # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                header_links = extract_links_from_element(header)
                footer_links = extract_links_from_element(footer)
                nav_links = []
                for nav in independent_navs:
                    nav_links.extend(extract_links_from_element(nav))
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã¨é™¤å»
                def extract_url_pattern(url: str, base_url: str = "") -> str:
                    """URLã‹ã‚‰ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                    try:
                        parsed = urlparse(url)
                        base_parsed = urlparse(base_url)
                        
                        # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹éƒ¨åˆ†ã‚’é™¤å¤–
                        base_path = base_parsed.path.strip('/')
                        full_path = parsed.path.strip('/')
                        
                        # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é™¤å¤–
                        if base_path and full_path.startswith(base_path):
                            relative_path = full_path[len(base_path):].strip('/')
                        else:
                            relative_path = full_path
                        
                        if relative_path:
                            parts = relative_path.split('/')
                            if len(parts) >= 2:
                                pattern_parts = parts[:-1] + ['*']
                                return '/' + '/'.join(pattern_parts) + '/'
                        
                        return parsed.path
                    except:
                        return url
                
                def detect_repeated_patterns(all_links: list, threshold: int = 10, base_url: str = "") -> set:
                    """åŒã˜ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ãŒé–¾å€¤å›æ•°ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                    pattern_counts = {}
                    url_to_pattern = {}
                    
                    for link in all_links:
                        url = link['url']
                        pattern = extract_url_pattern(url, base_url)
                        
                        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                        url_to_pattern[url] = pattern
                    
                    repeated_patterns = {pattern for pattern, count in pattern_counts.items() 
                                       if count >= threshold}
                    
                    urls_to_exclude = set()
                    for url, pattern in url_to_pattern.items():
                        if pattern in repeated_patterns:
                            urls_to_exclude.add(url)
                    
                    return urls_to_exclude
                
                # å…¨ãƒªãƒ³ã‚¯ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
                all_links = []
                seen_urls = set()
                
                for link_list in [header_links, footer_links, nav_links]:
                    for link in link_list:
                        url_key = link['url']
                        if url_key not in seen_urls:
                            seen_urls.add(url_key)
                            all_links.append(link)
                
                # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦é™¤å»ï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰
                urls_to_exclude = detect_repeated_patterns(all_links, threshold=10, base_url=url)
                filtered_links = [link for link in all_links 
                                if link['url'] not in urls_to_exclude]

                # è¦‹å‡ºã—æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆh2/h3 ã‚’çµ±åˆã—ãŸå˜ä¸€ãƒªã‚¹ãƒˆï¼‰
                def extract_headings(soup: BeautifulSoup) -> List[str]:
                    h2_list = [h.get_text(strip=True) for h in soup.find_all('h2') if h.get_text(strip=True)]
                    h3_list = [h.get_text(strip=True) for h in soup.find_all('h3') if h.get_text(strip=True)]
                    merged = h2_list + h3_list
                    # é‡è¤‡é™¤å»ã‚’ä¿æŒé †ã§è¡Œã†
                    seen = set()
                    unique_list: List[str] = []
                    for item in merged:
                        if item not in seen:
                            seen.add(item)
                            unique_list.append(item)
                    return unique_list[:100]

                async def fetch_headings_for_url(session: aiohttp.ClientSession, target_url: str) -> List[str]:
                    try:
                        async with session.get(target_url, headers=headers, timeout=ClientTimeout(total=15)) as resp:
                            if resp.status != 200:
                                return []
                            html_text = await resp.text()
                            page_soup = BeautifulSoup(html_text, "html.parser")
                            # ãƒã‚¤ã‚ºã«ãªã‚ŠãŒã¡ãªè¦ç´ ã¯è½ã¨ã™
                            for t in page_soup(["script", "style", "noscript"]):
                                t.decompose()
                            return extract_headings(page_soup)
                    except Exception:
                        return []

                # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é™å®šã—ã¦ä¸¦è¡Œã§è¦‹å‡ºã—ã‚’å–å¾—
                base_domain = urlparse(url).netloc.split(':')[0].lower()
                def is_same_domain(target: str) -> bool:
                    try:
                        netloc = urlparse(target).netloc.split(':')[0].lower()
                        # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚è¨±å¯ï¼ˆexample.com ã¨ www.example.com ãªã©ï¼‰
                        return netloc == base_domain or netloc.endswith('.' + base_domain)
                    except Exception:
                        return False

                max_fetch = 20
                concurrency = 5
                sem = asyncio.Semaphore(concurrency)

                eligible_indices = [i for i, l in enumerate(filtered_links) if is_same_domain(l['url'])][:max_fetch]

                async def bound_fetch(idx: int, target_url: str):
                    async with sem:
                        return idx, await fetch_headings_for_url(session, target_url)

                tasks = [asyncio.create_task(bound_fetch(i, filtered_links[i]['url'])) for i in eligible_indices]
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    for res in results:
                        if isinstance(res, tuple):
                            idx, headings = res
                            filtered_links[idx]["content_headings"] = headings
                # æœªä»˜ä¸ã®ãƒªãƒ³ã‚¯ã«ã¯ç©ºé…åˆ—ã‚’è¨­å®š
                for link_item in filtered_links:
                    if "content_headings" not in link_item:
                        link_item["content_headings"] = []
                
                logger.info(f"Successfully extracted {len(filtered_links)} links from {url}")
                
                # çµæœã‚’è¿”ã™
                return json.dumps({
                    "base_url": url,
                    "total_links": len(all_links),
                    "filtered_links": len(filtered_links),
                    "links": filtered_links,
                    "sections": {
                        "header_links": len(header_links),
                        "footer_links": len(footer_links),
                        "nav_links": len(nav_links)
                    }
                }, ensure_ascii=False)
                
    except Exception as e:
        logger.exception(f"Error in extract_site_links: {e}")
        return json.dumps({
            "error": str(e),
            "base_url": url,
            "links": []
        }, ensure_ascii=False)


@mcp.tool()
async def extract_site_links_with_playwright(url: str) -> str:
    """
    Playwrightã‚’ä½¿ç”¨ã—ã¦JavaScript/SPA/Reactã‚µã‚¤ãƒˆã‹ã‚‰header/footer/navã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    å‹•çš„ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å¯¾è±¡ã‚µã‚¤ãƒˆã®URLï¼ˆä¾‹ï¼šã€Œhttps://www.goldsgym.jpã€ï¼‰
    
    Returns:
        ã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯æƒ…å ±ã®JSONæ–‡å­—åˆ—ã€‚å„ãƒªãƒ³ã‚¯ã«ã¯text, urlã€content_headings(è¦‹å‡ºã—ãƒªã‚¹ãƒˆ)ãŒå«ã¾ã‚Œã‚‹
    """
    logger.info(f"extract_site_links_with_playwright called with url={url}")
    
    if not PLAYWRIGHT_AVAILABLE:
        error_msg = "Playwright is not available. Please install it with: pip install playwright && playwright install"
        logger.error(error_msg)
        return json.dumps({
            "error": error_msg,
            "base_url": url,
            "links": []
        }, ensure_ascii=False)
    
    if not async_playwright:
        return json.dumps({"error": "Playwright not available", "base_url": url, "links": []}, ensure_ascii=False)
    
    # PDFã‚’äº‹å‰ã«é™¤å¤–
    if url.lower().endswith('.pdf'):
        logger.warning(f"Skipping PDF: {url}")
        return json.dumps({"error": "PDF files are not supported", "base_url": url, "links": []}, ensure_ascii=False)
    
    async with browser_semaphore:  # ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
        browser = None
        context = None
        page = None
        try:
            logger.debug(f"Starting Playwright link extraction for: {url}")
            async with async_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--single-process']
                )
                # Contextã‚’ä½œæˆ
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                # User-Agentã‚’è¨­å®š
                await page.set_extra_http_headers({
                    "User-Agent": (
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    ),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                    "Accept-Encoding": "gzip, deflate",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Cache-Control": "max-age=0"
                })
                
                # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                logger.debug(f"Navigating to {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
                # SPAã®åˆæœŸãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å¾…ã¤
                logger.debug("Waiting for network idle")
                await page.wait_for_load_state("networkidle")
            
                # å‹•çš„è¦ç´ ã®å‡ºç¾ã‚’å¾…ã¤
                try:
                    await page.wait_for_selector(
                        "header, nav, footer, [role='navigation'], .header, .navbar, .navigation",
                        timeout=10000
                    )
                    logger.debug("Navigation elements found")
                except:
                    logger.warning(f"No navigation elements found immediately for {url}")
                
                # è¿½åŠ ã®å¾…æ©Ÿï¼ˆå‹•çš„ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã®ãŸã‚ï¼‰
                await page.wait_for_timeout(5000)
                
                # JavaScriptã§ãƒªãƒ³ã‚¯æ•°ã‚’äº‹å‰ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                js_link_info = await page.evaluate("""
                    () => {
                        return {
                            total: document.querySelectorAll('a').length,
                            inHeader: document.querySelectorAll('header a, .header a').length,
                            inNav: document.querySelectorAll('nav a, .nav a, .navbar a').length,
                            inFooter: document.querySelectorAll('footer a, .footer a').length,
                            hasHeader: !!document.querySelector('header, .header'),
                            hasNav: !!document.querySelector('nav, .nav, .navbar'),
                            hasFooter: !!document.querySelector('footer, .footer')
                        }
                    }
                """)
                logger.debug(f"Link info: {js_link_info}")
                
                # HTMLã‚’å–å¾—
                html = await page.content()
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(html, "html.parser")
            
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
            for tag in soup(["script", "style", "noscript"]):
                tag.decompose()
            
            # header/footer/navè¦ç´ ã‚’æ¢ã™ï¼ˆã‚»ãƒ¬ã‚¯ã‚¿ã‚’æ‹¡å¼µï¼‰
            header = (soup.find("header") or 
                     soup.select_one('[role="banner"]') or
                     soup.select_one('.header') or
                     soup.select_one('#header'))
            
            footer = (soup.find("footer") or 
                     soup.select_one('[role="contentinfo"]') or
                     soup.select_one('.footer') or
                     soup.select_one('#footer'))
            
            # ç‹¬ç«‹ã—ãŸnavè¦ç´ ã‚’æ¢ã™ï¼ˆheaderã®å¤–ã«ã‚ã‚‹ã‚‚ã®ï¼‰
            nav_elements = soup.find_all("nav")
            nav_elements.extend(soup.select('.nav, .navbar, .navigation'))
            
            independent_navs = []
            if header and isinstance(header, Tag):
                header_navs = header.find_all("nav")
                header_navs.extend(header.select('.nav, .navbar, .navigation'))
                independent_navs = [nav for nav in nav_elements if nav not in header_navs]
            else:
                independent_navs = nav_elements
            
            # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹å†…éƒ¨é–¢æ•°
            def extract_links_from_element(element):
                if element is None:
                    return []
                
                links = []
                for link in element.find_all('a', href=True):
                    text = link.get_text(strip=True)
                    href = link['href']
                    
                    # tel:, mailto:, javascript: ãªã©ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if href.startswith(('tel:', 'mailto:', 'javascript:', '#')):
                        continue
                    
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    absolute_url = urljoin(url, href)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿å‡¦ç†
                    if text:
                        links.append({
                            'text': text,
                            'url': absolute_url
                        })
                return links
            
            # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            header_links = extract_links_from_element(header)
            footer_links = extract_links_from_element(footer)
            nav_links = []
            for nav in independent_navs:
                nav_links.extend(extract_links_from_element(nav))
            
            logger.debug(f"Extracted - Header: {len(header_links)}, Footer: {len(footer_links)}, Nav: {len(nav_links)}")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã¨é™¤å»ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒï¼‰
            def extract_url_pattern(url: str, base_url: str = "") -> str:
                """URLã‹ã‚‰ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                try:
                    parsed = urlparse(url)
                    base_parsed = urlparse(base_url)
                    
                    # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹éƒ¨åˆ†ã‚’é™¤å¤–
                    base_path = base_parsed.path.strip('/')
                    full_path = parsed.path.strip('/')
                    
                    # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é™¤å¤–
                    if base_path and full_path.startswith(base_path):
                        relative_path = full_path[len(base_path):].strip('/')
                    else:
                        relative_path = full_path
                    
                    if relative_path:
                        parts = relative_path.split('/')
                        if len(parts) >= 2:
                            pattern_parts = parts[:-1] + ['*']
                            return '/' + '/'.join(pattern_parts) + '/'
                    
                    return parsed.path
                except:
                    return url
            
            def detect_repeated_patterns(all_links: list, threshold: int = 10, base_url: str = "") -> set:
                """åŒã˜ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ãŒé–¾å€¤å›æ•°ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                pattern_counts = {}
                url_to_pattern = {}
                
                for link in all_links:
                    url = link['url']
                    pattern = extract_url_pattern(url, base_url)
                    
                    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                    url_to_pattern[url] = pattern
                
                repeated_patterns = {pattern for pattern, count in pattern_counts.items() 
                                   if count >= threshold}
                
                urls_to_exclude = set()
                for url, pattern in url_to_pattern.items():
                    if pattern in repeated_patterns:
                        urls_to_exclude.add(url)
                
                return urls_to_exclude
            
            # å…¨ãƒªãƒ³ã‚¯ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
            all_links = []
            seen_urls = set()
            
            for link_list in [header_links, footer_links, nav_links]:
                for link in link_list:
                    url_key = link['url']
                    if url_key not in seen_urls:
                        seen_urls.add(url_key)
                        all_links.append(link)
            
            # ãƒªãƒ³ã‚¯ãŒ0ä»¶ã®å ´åˆã®è¿½åŠ å‡¦ç†
            if len(all_links) == 0:
                logger.warning("No links extracted from BeautifulSoup. Trying JavaScript extraction...")
                
                # JavaScriptã§ç›´æ¥ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                if async_playwright:
                    async with async_playwright() as p2:
                        browser2 = await p2.chromium.launch(headless=True, args=['--no-sandbox', '--single-process'])
                        context2 = await browser2.new_context()
                        page2 = await context2.new_page()
                    
                        await page2.goto(url, wait_until="networkidle", timeout=30000)
                        await page2.wait_for_timeout(5000)
                        
                        js_links = await page2.evaluate("""
                        () => {
                            const foundLinks = [];
                            const seen = new Set();
                            
                            document.querySelectorAll('a').forEach(link => {
                                if (link.href && 
                                    !link.href.startsWith('tel:') && 
                                    !link.href.startsWith('mailto:') &&
                                    !link.href.startsWith('javascript:') &&
                                    !seen.has(link.href)) {
                                    seen.add(link.href);
                                    foundLinks.push({
                                        text: link.innerText.trim() || link.textContent.trim() || 'No text',
                                        url: link.href
                                    });
                                }
                            });
                            
                            return foundLinks;
                        }
                        """)
                        
                        await page2.close()
                        await context2.close()
                        await browser2.close()
                        
                        all_links = js_links
                        logger.info(f"JavaScript extraction found {len(all_links)} links")
            
            # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦é™¤å»ï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰
            urls_to_exclude = detect_repeated_patterns(all_links, threshold=10, base_url=url)
            filtered_links = [link for link in all_links 
                            if link['url'] not in urls_to_exclude]
            
            # è¦‹å‡ºã—æŠ½å‡ºã¯ç°¡æ˜“ç‰ˆï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
            for link_item in filtered_links:
                link_item["content_headings"] = []
            
            logger.info(f"Successfully extracted {len(filtered_links)} links from {url}")
            
            # çµæœã‚’è¿”ã™
            return json.dumps({
                "base_url": url,
                "total_links": len(all_links),
                "filtered_links": len(filtered_links),
                "links": filtered_links,
                "sections": {
                    "header_links": len(header_links),
                    "footer_links": len(footer_links),
                    "nav_links": len(nav_links)
                },
                "debug_info": js_link_info if 'js_link_info' in locals() else {}
            }, ensure_ascii=False)
                
        except Exception as e:
            logger.exception(f"Error in extract_site_links_with_playwright: {e}")
            return json.dumps({
                "error": str(e),
                "base_url": url,
                "links": []
            }, ensure_ascii=False)
        finally:
            # å®Œå…¨ãªçµ‚äº†å‡¦ç†
            try:
                if page:
                    await page.close()
            except:
                pass
            try:
                if context:
                    await context.close()
            except:
                pass
            try:
                if browser:
                    await browser.close()
                    await asyncio.sleep(1)
            except:
                pass


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Rakuraku Media School tools
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@mcp.tool()
async def rakuraku_school_list(
    keyword: str = "",
    per_page: int = 20,
    page: int = 1,
    status: str = "publish,draft",
    include_custom_fields: bool = False,
) -> str:
    """
    Rakuraku Media School ã®ã€Œschool-listã€ã‚«ã‚¹ã‚¿ãƒ æŠ•ç¨¿ã‚’æ¤œç´¢ãƒ»ä¸€è¦§è¡¨ç¤ºã—ã¾ã™ã€‚
    WordPress ç®¡ç†ç”»é¢ï¼ˆ/wp-admin/edit.php?post_type=school-listï¼‰ã¨åŒç­‰ã®æƒ…å ±ã‚’å–å¾—ã§ãã¾ã™ã€‚
    
    Args:
        keyword: ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«å¯¾ã™ã‚‹æ¤œç´¢èª
        per_page: å–å¾—ä»¶æ•° (1-100)
        page: ãƒšãƒ¼ã‚¸ç•ªå· (1ä»¥ä¸Š)
        status: å–å¾—ã™ã‚‹æŠ•ç¨¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆä¾‹: "publish", "draft", "publish,draft"ï¼‰
        include_custom_fields: True ã®å ´åˆã¯ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤º
    """
    logger.info(
        "[Rakuraku] school-list æ¤œç´¢ keyword=%s, per_page=%s, page=%s",
        keyword,
        per_page,
        page,
    )
    
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    per_page = max(1, min(per_page, 100))
    page = max(1, page)
    
    params: Dict[str, Any] = {
        "per_page": per_page,
        "page": page,
        "status": _rakuraku_build_status_param(status),
        "context": "edit",
        "orderby": "date",
        "order": "desc",
    }
    
    if keyword:
        params["search"] = keyword
    if not include_custom_fields:
        params["_fields"] = RAKURAKU_DEFAULT_FIELDS
    
    try:
        posts, headers = await _rakuraku_wp_get(RAKURAKU_POST_TYPE, params=params)
    except RuntimeError as exc:
        logger.error("[Rakuraku] school-list å–å¾—å¤±æ•—: %s", exc)
        return f"ã‚¨ãƒ©ãƒ¼: {exc}"
    
    if not isinstance(posts, list):
        return "ã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§ã™ã€‚"
    if not posts:
        return "æŒ‡å®šæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ school-list æŠ•ç¨¿ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    total_posts = headers.get("X-WP-Total", "unknown")
    total_pages = headers.get("X-WP-TotalPages", "unknown")
    
    lines = [
        f"ğŸ“š Rakuraku Media School school-list æŠ•ç¨¿ ({len(posts)}ä»¶)",
        f"   page {page}/{total_pages} / total posts: {total_posts}",
        ""
    ]
    
    for post in posts:
        lines.append(_rakuraku_format_summary(post, include_fields=include_custom_fields))
        lines.append("")
    
    return "\n".join(lines).strip()


@mcp.tool()
async def rakuraku_school_detail(identifier: str) -> str:
    """
    school-list æŠ•ç¨¿ã‚’ ID / slug / ã‚¿ã‚¤ãƒˆãƒ«ã§ç‰¹å®šã—ã€å…¨æ–‡ã¨ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å–å¾—ã—ã¾ã™ã€‚
    
    Args:
        identifier: æŠ•ç¨¿IDï¼ˆæ•°å€¤ï¼‰ã€ã¾ãŸã¯ slug / ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨
    """
    identifier = (identifier or "").strip()
    if not identifier:
        return "identifier ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ï¼ˆID, slug, ã‚¿ã‚¤ãƒˆãƒ«ã®ä¸€éƒ¨ãªã©ï¼‰ã€‚"
    
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    logger.info("[Rakuraku] school-list è©³ç´°å–å¾— identifier=%s", identifier)
    
    try:
        post = await _rakuraku_find_post(identifier)
    except RuntimeError as exc:
        logger.error("[Rakuraku] school-list è©³ç´°å–å¾—å¤±æ•—: %s", exc)
        return f"ã‚¨ãƒ©ãƒ¼: {exc}"
    
    if not post:
        return f"identifier '{identifier}' ã«ä¸€è‡´ã™ã‚‹ school-list æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    return _rakuraku_format_detail(post)


@mcp.tool()
async def rakuraku_create_school_post(
    title: str,
    content: str = "",
    status: str = "draft",
    fields_json: str = "",
    excerpt: str = "",
    slug: str = ""
) -> str:
    """
    school-list ã‚«ã‚¹ã‚¿ãƒ æŠ•ç¨¿ã‚’æ–°è¦ä½œæˆã—ã¾ã™ã€‚
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    clean_title = (title or "").strip()
    if not clean_title:
        return "ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    
    payload: Dict[str, Any] = {
        "title": clean_title,
        "status": _rakuraku_normalize_single_status(status),
    }
    if content:
        payload["content"] = content
    if excerpt:
        payload["excerpt"] = excerpt
    if slug:
        payload["slug"] = slug
    
    fields, error = _rakuraku_parse_fields_json(fields_json)
    if error:
        return error
    if fields:
        payload["meta"] = fields
    
    try:
        post = await _rakuraku_wp_post(RAKURAKU_POST_TYPE, payload)
    except RuntimeError as exc:
        logger.error("[Rakuraku] school-list ä½œæˆå¤±æ•—: %s", exc)
        return f"âŒ ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{exc}"
    
    return _rakuraku_format_post_action_result("âœ… school-list æŠ•ç¨¿ã‚’ä½œæˆã—ã¾ã—ãŸ", post)


@mcp.tool()
async def rakuraku_update_school_post(
    post_id: int,
    title: str = "",
    content: str = "",
    status: str = "",
    fields_json: str = "",
    excerpt: str = "",
    slug: str = ""
) -> str:
    """
    school-list æŠ•ç¨¿ã®ã‚¿ã‚¤ãƒˆãƒ« / æœ¬æ–‡ / ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ / ãƒ¡ã‚¿æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    payload: Dict[str, Any] = {}
    if title:
        payload["title"] = title
    if content:
        payload["content"] = content
    if excerpt:
        payload["excerpt"] = excerpt
    if slug:
        payload["slug"] = slug
    if status:
        payload["status"] = _rakuraku_normalize_single_status(status)
    
    fields, error = _rakuraku_parse_fields_json(fields_json)
    if error:
        return error
    if fields:
        payload.setdefault("meta", {}).update(fields)
    
    if not payload:
        return "æ›´æ–°é …ç›®ã‚’1ã¤ä»¥ä¸ŠæŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    
    try:
        post = await _rakuraku_wp_post(f"{RAKURAKU_POST_TYPE}/{post_id}", payload)
    except RuntimeError as exc:
        logger.error("[Rakuraku] school-list æ›´æ–°å¤±æ•—: %s", exc)
        return f"âŒ æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{exc}"
    
    return _rakuraku_format_post_action_result("âœ… school-list æŠ•ç¨¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ", post)


@mcp.tool()
async def rakuraku_media_posts(
    keyword: str = "",
    category_id: int = 0,
    status: str = "publish,draft",
    per_page: int = 20,
    page: int = 1,
    include_custom_fields: bool = False,
) -> str:
    """
    Rakuraku Media School ã®é€šå¸¸æŠ•ç¨¿ï¼ˆ/wp-admin/edit.phpï¼‰ã‚’æ¤œç´¢ã—ã¾ã™ã€‚
    
    Args:
        keyword: æ¤œç´¢èª
        category_id: ã‚«ãƒ†ã‚´ãƒªãƒ¼IDï¼ˆ0ã§ç„¡è¦–ï¼‰
        status: æŠ•ç¨¿ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼ˆä¾‹: "publish", "draft", "publish,draft"ï¼‰
        per_page: å–å¾—ä»¶æ•° (1-100)
        page: ãƒšãƒ¼ã‚¸ç•ªå·
        include_custom_fields: ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼è¡¨ç¤ºã™ã‚‹ã‹
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    per_page = max(1, min(per_page, 100))
    page = max(1, page)
    status_param = _rakuraku_build_status_param(status)
    
    params: Dict[str, Any] = {
        "per_page": per_page,
        "page": page,
        "status": status_param,
        "context": "edit",
        "orderby": "date",
        "order": "desc",
    }
    if keyword:
        params["search"] = keyword
    if category_id > 0:
        params["categories"] = category_id
    if not include_custom_fields:
        params["_fields"] = RAKURAKU_DEFAULT_FIELDS
    
    logger.info(
        "[Rakuraku] posts æ¤œç´¢ keyword=%s category=%s status=%s page=%s",
        keyword,
        category_id or "any",
        status_param,
        page,
    )
    
    try:
        posts, headers = await _rakuraku_wp_get("posts", params=params)
    except RuntimeError as exc:
        logger.error("[Rakuraku] posts å–å¾—å¤±æ•—: %s", exc)
        return f"ã‚¨ãƒ©ãƒ¼: {exc}"
    
    if not isinstance(posts, list):
        return "ã‚¨ãƒ©ãƒ¼: äºˆæœŸã—ãªã„ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã§ã™ã€‚"
    if not posts:
        return "æŒ‡å®šæ¡ä»¶ã«ä¸€è‡´ã™ã‚‹æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    total_posts = headers.get("X-WP-Total", "unknown")
    total_pages = headers.get("X-WP-TotalPages", "unknown")
    
    lines = [
        f"ğŸ“° Rakuraku Media School é€šå¸¸æŠ•ç¨¿ ({len(posts)}ä»¶)",
        f"   page {page}/{total_pages} / total posts: {total_posts}",
        ""
    ]
    for post in posts:
        lines.append(_rakuraku_format_summary(post, include_fields=include_custom_fields))
        lines.append("")
    
    return "\n".join(lines).strip()


@mcp.tool()
async def rakuraku_create_media_post(
    title: str,
    content: str = "",
    status: str = "draft",
    fields_json: str = "",
    excerpt: str = "",
    slug: str = ""
) -> str:
    """
    é€šå¸¸æŠ•ç¨¿ï¼ˆpostï¼‰ã‚’æ–°è¦ä½œæˆã—ã¾ã™ã€‚
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    clean_title = (title or "").strip()
    if not clean_title:
        return "ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    
    payload: Dict[str, Any] = {
        "title": clean_title,
        "status": _rakuraku_normalize_single_status(status),
    }
    if content:
        payload["content"] = content
    if excerpt:
        payload["excerpt"] = excerpt
    if slug:
        payload["slug"] = slug
    
    fields, error = _rakuraku_parse_fields_json(fields_json)
    if error:
        return error
    if fields:
        payload["meta"] = fields
    
    try:
        post = await _rakuraku_wp_post("posts", payload)
    except RuntimeError as exc:
        logger.error("[Rakuraku] posts ä½œæˆå¤±æ•—: %s", exc)
        return f"âŒ ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{exc}"
    
    return _rakuraku_format_post_action_result("âœ… é€šå¸¸æŠ•ç¨¿ã‚’ä½œæˆã—ã¾ã—ãŸ", post)


@mcp.tool()
async def rakuraku_update_media_post(
    post_id: int,
    title: str = "",
    content: str = "",
    status: str = "",
    fields_json: str = "",
    excerpt: str = "",
    slug: str = ""
) -> str:
    """
    é€šå¸¸æŠ•ç¨¿ã®ã‚¿ã‚¤ãƒˆãƒ« / æœ¬æ–‡ / ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ / ãƒ¡ã‚¿æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    payload: Dict[str, Any] = {}
    if title:
        payload["title"] = title
    if content:
        payload["content"] = content
    if excerpt:
        payload["excerpt"] = excerpt
    if slug:
        payload["slug"] = slug
    if status:
        payload["status"] = _rakuraku_normalize_single_status(status)
    
    fields, error = _rakuraku_parse_fields_json(fields_json)
    if error:
        return error
    if fields:
        payload.setdefault("meta", {}).update(fields)
    
    if not payload:
        return "æ›´æ–°é …ç›®ã‚’1ã¤ä»¥ä¸ŠæŒ‡å®šã—ã¦ãã ã•ã„ã€‚"
    
    try:
        post = await _rakuraku_wp_post(f"posts/{post_id}", payload)
    except RuntimeError as exc:
        logger.error("[Rakuraku] posts æ›´æ–°å¤±æ•—: %s", exc)
        return f"âŒ æ›´æ–°ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\n{exc}"
    
    return _rakuraku_format_post_action_result("âœ… é€šå¸¸æŠ•ç¨¿ã‚’æ›´æ–°ã—ã¾ã—ãŸ", post)


@mcp.tool()
async def rakuraku_update_school_fields(
    post_id: int,
    fields_json: str,
    container: str = "meta",
    wrap_payload: bool = True,
) -> str:
    """
    school-list æŠ•ç¨¿ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰/ãƒ¡ã‚¿æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    
    Args:
        post_id: æ›´æ–°å¯¾è±¡ã®æŠ•ç¨¿ID
        fields_json: {"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å": "å€¤"} å½¢å¼ã®JSONæ–‡å­—åˆ—
        container: custom_fields / meta / acf ã®ã„ãšã‚Œã‹ï¼ˆwrap_payload=True ã®å ´åˆï¼‰
        wrap_payload: True ã§ JSON ã‚’ container å†…ã«åŒ…ã‚“ã§é€ä¿¡ã€False ã§ JSON ã‚’ãã®ã¾ã¾é€ä¿¡
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    return await _rakuraku_handle_update_tool(
        post_type=RAKURAKU_POST_TYPE,
        post_id=post_id,
        fields_json=fields_json,
        container=container,
        wrap_payload=wrap_payload,
        label=RAKURAKU_POST_TYPE,
    )


@mcp.tool()
async def rakuraku_update_media_fields(
    post_id: int,
    fields_json: str,
    container: str = "meta",
    wrap_payload: bool = True,
) -> str:
    """
    é€šå¸¸æŠ•ç¨¿ï¼ˆ/wp-admin/edit.phpï¼‰ã«å¯¾ã—ã¦ã‚«ã‚¹ã‚¿ãƒ ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚„ãƒ¡ã‚¿æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚
    
    Args:
        post_id: æŠ•ç¨¿ID
        fields_json: {"ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å": "å€¤"} å½¢å¼ã®JSONæ–‡å­—åˆ—
        container: custom_fields / meta / acf ï¼ˆwrap_payload=True ã®å ´åˆï¼‰
        wrap_payload: True ã®å ´åˆã¯ container ä»˜ãã§é€ä¿¡ã€False ã§ä»»æ„ã®payloadã‚’é€ä¿¡
    """
    if not _rakuraku_credentials_ready():
        return _rakuraku_missing_credentials_message()
    
    return await _rakuraku_handle_update_tool(
        post_type="posts",
        post_id=post_id,
        fields_json=fields_json,
        container=container,
        wrap_payload=wrap_payload,
        label="posts",
    )


@mcp.tool()
async def search_cloud_gym_introduce(
    keyword: str,
    per_page: int = 20,
    page: int = 1,
    include_terms: bool = False,
) -> str:
    """
    Cloud GYMã‚µã‚¤ãƒˆã®introduceæŠ•ç¨¿ã‚’æ¤œç´¢ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚

    Args:
        keyword: WordPress REST APIã®searchãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ¸¡ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        per_page: å–å¾—ä»¶æ•° (1-100)
        page: ãƒšãƒ¼ã‚¸ç•ªå· (1ä»¥ä¸Š)
        include_terms: ã‚¿ã‚¯ã‚½ãƒãƒŸãƒ¼æƒ…å ±ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹

    Returns:
        çµæœã‚’è¡¨ã™JSONæ–‡å­—åˆ—
    """
    if not keyword:
        return json.dumps(
            {
                "success": False,
                "error": "keyword_required",
                "message": "æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚",
            },
            ensure_ascii=False,
        )

    per_page = max(1, min(per_page, 100))
    page = max(1, page)

    params: Dict[str, Any] = {
        "search": keyword,
        "per_page": per_page,
        "page": page,
        "status": "publish",
        "_fields": CLOUD_GYM_DEFAULT_FIELDS,
    }

    if include_terms:
        params["_embed"] = ""
        if "_embedded" not in params["_fields"]:
            params["_fields"] = f"{params['_fields']},_embedded"

    logger.info(
        "[CloudGym] introduceæ¤œç´¢: %s (page=%s, per_page=%s)",
        keyword,
        page,
        per_page,
    )
    logger.debug("[CloudGym] params=%s", params)

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(CLOUD_GYM_API_ENDPOINT, params=params) as response:
                if response.status == 200:
                    raw_posts = await response.json()
                    total_posts = response.headers.get("X-WP-Total", "unknown")
                    total_pages = response.headers.get("X-WP-TotalPages", "unknown")

                    result: Dict[str, Any] = {
                        "success": True,
                        "site": CLOUD_GYM_BASE_URL,
                        "post_type": CLOUD_GYM_POST_TYPE,
                        "search_keyword": keyword,
                        "count": len(raw_posts),
                        "pagination": {
                            "current_page": page,
                            "per_page": per_page,
                            "total_posts": total_posts,
                            "total_pages": total_pages,
                        },
                        "posts": _cloud_gym_normalize_posts(raw_posts),
                    }

                    if include_terms:
                        taxonomy_terms = _cloud_gym_extract_taxonomy_terms(raw_posts)
                        if taxonomy_terms:
                            result["taxonomy_terms"] = taxonomy_terms

                    logger.info("[CloudGym] introduceå–å¾—: %sä»¶", len(raw_posts))
                    return json.dumps(result, ensure_ascii=False, indent=2)

                error_payload = await _cloud_gym_extract_error_payload(response)
                logger.error(
                    "[CloudGym] APIã‚¨ãƒ©ãƒ¼ (%s): %s", response.status, error_payload
                )
                return json.dumps(
                    {
                        "success": False,
                        "error": "api_error",
                        "status": response.status,
                        "details": error_payload,
                    },
                    ensure_ascii=False,
                )

    except aiohttp.ClientError as exc:
        logger.error("[CloudGym] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: %s", exc)
        return json.dumps(
            {
                "success": False,
                "error": "network_error",
                "message": f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(exc)}",
            },
            ensure_ascii=False,
        )
    except Exception as exc:
        logger.exception("[CloudGym] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: %s", exc)
        return json.dumps(
            {
                "success": False,
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(exc)}",
            },
            ensure_ascii=False,
        )


@mcp.tool()
async def search_cloud_gym_posts(
    keyword: str = "",
    per_page: int = 20,
    page: int = 1,
    include_terms: bool = False,
) -> str:
    """
    Cloud GYMã‚µã‚¤ãƒˆã®é€šå¸¸ã®æŠ•ç¨¿ï¼ˆpostsï¼‰ã‚’æ¤œç´¢ãƒ»å–å¾—ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã€‚
    ã‚«ã‚¹ã‚¿ãƒ æŠ•ç¨¿ã‚¿ã‚¤ãƒ—ã§ã¯ãªãã€WordPressã®æ¨™æº–æŠ•ç¨¿ã‚’å¯¾è±¡ã¨ã—ã¾ã™ã€‚

    Args:
        keyword: WordPress REST APIã®searchãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æ¸¡ã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆç©ºæ–‡å­—åˆ—ã®å ´åˆã¯å…¨ä»¶å–å¾—ï¼‰
        per_page: å–å¾—ä»¶æ•° (1-100)
        page: ãƒšãƒ¼ã‚¸ç•ªå· (1ä»¥ä¸Š)
        include_terms: ã‚¿ã‚¯ã‚½ãƒãƒŸãƒ¼æƒ…å ±ï¼ˆã‚«ãƒ†ã‚´ãƒªãƒ¼ã€ã‚¿ã‚°ãªã©ï¼‰ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹

    Returns:
        çµæœã‚’è¡¨ã™JSONæ–‡å­—åˆ—
    """
    per_page = max(1, min(per_page, 100))
    page = max(1, page)

    # é€šå¸¸ã®æŠ•ç¨¿ç”¨ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    posts_endpoint = f"{CLOUD_GYM_BASE_URL.rstrip('/')}/wp-json/wp/v2/posts"

    params: Dict[str, Any] = {
        "per_page": per_page,
        "page": page,
        "status": "publish",
        "_fields": CLOUD_GYM_DEFAULT_FIELDS,
    }

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿searchãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    if keyword:
        params["search"] = keyword

    if include_terms:
        params["_embed"] = ""
        if "_embedded" not in params["_fields"]:
            params["_fields"] = f"{params['_fields']},_embedded"

    logger.info(
        "[CloudGym] postsæ¤œç´¢: %s (page=%s, per_page=%s)",
        keyword or "å…¨ä»¶",
        page,
        per_page,
    )
    logger.debug("[CloudGym] params=%s", params)

    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(posts_endpoint, params=params) as response:
                if response.status == 200:
                    raw_posts = await response.json()
                    total_posts = response.headers.get("X-WP-Total", "unknown")
                    total_pages = response.headers.get("X-WP-TotalPages", "unknown")

                    result: Dict[str, Any] = {
                        "success": True,
                        "site": CLOUD_GYM_BASE_URL,
                        "post_type": "posts",
                        "search_keyword": keyword or None,
                        "count": len(raw_posts),
                        "pagination": {
                            "current_page": page,
                            "per_page": per_page,
                            "total_posts": total_posts,
                            "total_pages": total_pages,
                        },
                        "posts": _cloud_gym_normalize_posts(raw_posts),
                    }

                    if include_terms:
                        taxonomy_terms = _cloud_gym_extract_taxonomy_terms(raw_posts)
                        if taxonomy_terms:
                            result["taxonomy_terms"] = taxonomy_terms

                    logger.info("[CloudGym] postså–å¾—: %sä»¶", len(raw_posts))
                    return json.dumps(result, ensure_ascii=False, indent=2)

                error_payload = await _cloud_gym_extract_error_payload(response)
                logger.error(
                    "[CloudGym] APIã‚¨ãƒ©ãƒ¼ (%s): %s", response.status, error_payload
                )
                return json.dumps(
                    {
                        "success": False,
                        "error": "api_error",
                        "status": response.status,
                        "details": error_payload,
                    },
                    ensure_ascii=False,
                )

    except aiohttp.ClientError as exc:
        logger.error("[CloudGym] ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: %s", exc)
        return json.dumps(
            {
                "success": False,
                "error": "network_error",
                "message": f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(exc)}",
            },
            ensure_ascii=False,
        )
    except Exception as exc:
        logger.exception("[CloudGym] äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: %s", exc)
        return json.dumps(
            {
                "success": False,
                "error": "unexpected_error",
                "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(exc)}",
            },
            ensure_ascii=False,
        )


def _cloud_gym_normalize_posts(posts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    normalized = []
    for post in posts:
        normalized.append(
            {
                "id": post.get("id"),
                "title": _cloud_gym_extract_text_field(post.get("title")),
                "excerpt": _cloud_gym_extract_text_field(post.get("excerpt")),
                "date": post.get("date"),
                "link": post.get("link"),
                "slug": post.get("slug"),
            }
        )
    return normalized


def _cloud_gym_extract_text_field(field: Any) -> str:
    if isinstance(field, dict):
        return field.get("rendered") or field.get("raw") or ""
    if isinstance(field, str):
        return field
    return ""


def _cloud_gym_extract_taxonomy_terms(posts: List[Dict[str, Any]]) -> Dict[str, Any]:
    taxonomy_terms: Dict[str, Dict[int, Dict[str, Any]]] = {}

    for post in posts:
        embedded_terms = post.get("_embedded", {}).get("wp:term", [])
        for term_group in embedded_terms:
            for term in term_group:
                taxonomy = term.get("taxonomy")
                term_id = term.get("id")
                if taxonomy and term_id is not None:
                    taxonomy_terms.setdefault(taxonomy, {})
                    if term_id not in taxonomy_terms[taxonomy]:
                        taxonomy_terms[taxonomy][term_id] = {
                            "id": term_id,
                            "name": term.get("name", ""),
                            "slug": term.get("slug", ""),
                            "description": term.get("description", ""),
                            "count": term.get("count", 0),
                        }

    result: Dict[str, Any] = {}
    for taxonomy, terms in taxonomy_terms.items():
        result[taxonomy] = {
            "total": len(terms),
            "terms": list(terms.values()),
        }
    return result


async def _cloud_gym_extract_error_payload(response: aiohttp.ClientResponse) -> Any:
    try:
        return await response.json()
    except aiohttp.ContentTypeError:
        return await response.text()


@mcp.tool()
async def extract_store_ids_from_post(url: str) -> str:
    """
    Cloud GYMã®æŠ•ç¨¿ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ã®ã‚¹ãƒˆã‚¢IDï¼ˆstore_XXå½¢å¼ï¼‰ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    æŠ•ç¨¿æœ¬æ–‡ã‚’èª­ã¿è¾¼ã‚“ã§ã€idå±æ€§ãŒ"store_"ã§å§‹ã¾ã‚‹è¦ç´ ã‚’æ¢ã—ã¾ã™ã€‚

    Args:
        url: æŠ•ç¨¿ã®URLï¼ˆä¾‹ï¼šã€Œhttps://cloud-gym.com/personal-fitness/archives/575ã€ï¼‰

    Returns:
        æŠ½å‡ºã•ã‚ŒãŸã‚¹ãƒˆã‚¢IDã¨ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ä»˜ãURLã®JSONæ–‡å­—åˆ—
    """
    logger.info(f"extract_store_ids_from_post called with url={url}")
    
    try:
        # æŠ•ç¨¿ãƒšãƒ¼ã‚¸ã®HTMLã‚’å–å¾—
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/120.0.0.0 Safari/537.36"
            ),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=ClientTimeout(total=20)) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch page: {response.status}"
                    logger.error(error_msg)
                    return json.dumps({
                        "success": False,
                        "error": error_msg,
                        "url": url,
                        "store_ids": [],
                        "anchor_urls": []
                    }, ensure_ascii=False)
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # store_ã§å§‹ã¾ã‚‹IDã‚’æŒã¤è¦ç´ ã‚’æ¢ã™
                store_ids = set()
                
                # 1. idå±æ€§ãŒ"store_"ã§å§‹ã¾ã‚‹è¦ç´ ã‚’æ¢ã™
                for element in soup.find_all(id=re.compile(r'^store_\d+$')):
                    store_id = element.get('id', '')
                    if store_id:
                        store_ids.add(store_id)
                
                # 2. classå±æ€§ã«"store_"ã‚’å«ã‚€è¦ç´ ã‚‚æ¢ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                for element in soup.find_all(class_=re.compile(r'store_\d+')):
                    classes = element.get('class', [])
                    for cls in classes:
                        match = re.search(r'store_(\d+)', cls)
                        if match:
                            store_ids.add(f"store_{match.group(1)}")
                
                # 3. ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰ã‚‚æ¢ã™
                for element in soup.find_all(attrs={'data-store-id': True}):
                    store_id_attr = element.get('data-store-id', '')
                    if store_id_attr:
                        if not store_id_attr.startswith('store_'):
                            store_ids.add(f"store_{store_id_attr}")
                        else:
                            store_ids.add(store_id_attr)
                
                # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ä»˜ãURLã‚’ç”Ÿæˆ
                anchor_urls = []
                for store_id in sorted(store_ids):
                    anchor_urls.append(f"{url}#{store_id}")
                
                logger.info(f"Extracted {len(store_ids)} store IDs from {url}")
                
                result = {
                    "success": True,
                    "url": url,
                    "store_ids": sorted(list(store_ids)),
                    "anchor_urls": anchor_urls,
                    "count": len(store_ids)
                }
                
                return json.dumps(result, ensure_ascii=False, indent=2)
                
    except Exception as e:
        logger.exception(f"Error in extract_store_ids_from_post: {e}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "url": url,
            "store_ids": [],
            "anchor_urls": []
        }, ensure_ascii=False)


@mcp.tool()
async def generate_gym_introduction_email(
    region: str,
    per_page: int = 50,
    max_posts: int = 10
) -> str:
    """
    æŒ‡å®šã•ã‚ŒãŸåœ°åŸŸã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ æŠ•ç¨¿ã‚’æ¤œç´¢ã—ã€ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚
    æŠ•ç¨¿æœ¬æ–‡ã‚’èª­ã¿è¾¼ã‚“ã§å®Ÿéš›ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€
    ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ä»˜ãURLã‚’ç”Ÿæˆã—ã¦ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«åŸ‹ã‚è¾¼ã¿ã¾ã™ã€‚

    Args:
        region: æ¤œç´¢ã™ã‚‹åœ°åŸŸåï¼ˆä¾‹ï¼šã€Œæ±äº¬ã€ã€Œå¤§é˜ªã€ã€Œæ¨ªæµœã€ï¼‰
        per_page: 1å›ã®æ¤œç´¢ã§å–å¾—ã™ã‚‹ä»¶æ•° (1-100)
        max_posts: å‡¦ç†ã™ã‚‹æœ€å¤§æŠ•ç¨¿æ•°ï¼ˆå®Ÿéš›ã«ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ãŒå­˜åœ¨ã™ã‚‹æŠ•ç¨¿ã®ã¿ï¼‰

    Returns:
        ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨å‡¦ç†çµæœã®JSONæ–‡å­—åˆ—
    """
    logger.info(f"generate_gym_introduction_email called with region={region}")
    
    per_page = max(1, min(per_page, 100))
    max_posts = max(1, max_posts)
    
    try:
        # 1. åœ°åŸŸåã§æŠ•ç¨¿ã‚’æ¤œç´¢
        posts_endpoint = f"{CLOUD_GYM_BASE_URL.rstrip('/')}/wp-json/wp/v2/posts"
        params = {
            "search": region,
            "per_page": per_page,
            "page": 1,
            "status": "publish",
            "_fields": CLOUD_GYM_DEFAULT_FIELDS,
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(posts_endpoint, params=params) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch posts: {response.status}"
                    logger.error(error_msg)
                    return json.dumps({
                        "success": False,
                        "error": error_msg,
                        "region": region
                    }, ensure_ascii=False)
                
                raw_posts = await response.json()
                logger.info(f"Found {len(raw_posts)} posts for region: {region}")
        
        # 2. å„æŠ•ç¨¿ã‹ã‚‰ã‚¹ãƒˆã‚¢IDã‚’æŠ½å‡º
        valid_posts = []
        processed_count = 0
        
        for post in raw_posts:
            if processed_count >= max_posts:
                break
            
            post_link = post.get("link", "")
            if not post_link:
                continue
            
            # æŠ•ç¨¿ã‹ã‚‰ã‚¹ãƒˆã‚¢IDã‚’æŠ½å‡º
            store_result_json = await extract_store_ids_from_post(post_link)
            store_result = json.loads(store_result_json)
            
            if store_result.get("success") and store_result.get("store_ids"):
                # ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ãŒå®Ÿéš›ã«å­˜åœ¨ã™ã‚‹æŠ•ç¨¿
                valid_posts.append({
                    "title": _cloud_gym_extract_text_field(post.get("title")),
                    "url": post_link,
                    "store_ids": store_result.get("store_ids", []),
                    "anchor_urls": store_result.get("anchor_urls", [])
                })
                processed_count += 1
                logger.info(f"Valid post found: {post_link} ({len(store_result.get('store_ids', []))} stores)")
        
        # 3. ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”Ÿæˆ
        if not valid_posts:
            return json.dumps({
                "success": False,
                "error": "no_valid_posts",
                "message": f"åœ°åŸŸã€Œ{region}ã€ã§ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ãŒå­˜åœ¨ã™ã‚‹æŠ•ç¨¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚",
                "region": region,
                "searched_posts": len(raw_posts)
            }, ensure_ascii=False, indent=2)
        
        # ã‚¢ãƒ³ã‚«ãƒ¼ãƒªãƒ³ã‚¯ä»˜ãURLã‚’åé›†
        all_anchor_urls = []
        for post_info in valid_posts:
            all_anchor_urls.extend(post_info["anchor_urls"])
        
        # ãƒ¡ãƒ¼ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        url_section = "\n".join(all_anchor_urls)
        
        email_template = f"""çªç„¶ã®ã”é€£çµ¡å¤±ç¤¼ã„ãŸã—ã¾ã™ã€‚
ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ æ¯”è¼ƒãƒ¡ãƒ‡ã‚£ã‚¢ã€Œpersonal-fitnessã€ï¼ˆhttps://cloud-gym.com/personal-fitness/ï¼‰ã‚’é‹å–¶ã—ã¦ãŠã‚Šã¾ã™æ ªå¼ä¼šç¤¾Buildsã®é‡‘è—¤å„ªå¤ªã¨ç”³ã—ã¾ã™ã€‚

ã€Œpersonal-fitnessã€ã§ã¯ã€ãƒ¡ãƒ‡ã‚£ã‚¢ç«‹ã¡ä¸Šã’ã‹ã‚‰é †èª¿ã«PVæ•°ã‚’ä¼¸ã°ã—ã¦ãŠã‚Šã€å…¨å›½ã®ã‚¸ãƒ ãƒ»ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ 5,000åº—èˆ—ä»¥ä¸Šã‚’ç´¹ä»‹ã—ã¦ã„ã‚‹ãƒ¡ãƒ‡ã‚£ã‚¢ã¨ãªã‚Šã¾ã™ã€‚

äº‹å¾Œã®ã”é€£çµ¡ã¨ãªã£ã¦ã—ã¾ã„å¤§å¤‰æç¸®ã§ã™ãŒã€ã“ã®åº¦ã€å¼Šç¤¾ãƒ¡ãƒ‡ã‚£ã‚¢ã«ã¦è²´ç¤¾ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ã‚’ãŠã™ã™ã‚ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ã¨ã—ã¦ã”ç´¹ä»‹ã•ã›ã¦ã„ãŸã ãã¾ã—ãŸã®ã§ã”é€£çµ¡ã„ãŸã—ã¾ã—ãŸã€‚

ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
â–¼ã“ã®åº¦ã€è²´ç¤¾ãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚¸ãƒ ã‚’ã”ç´¹ä»‹ã•ã›ã¦ã„ãŸã ã„ãŸè¨˜äº‹â–¼
{url_section}
ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼

ãƒ¡ãƒ‡ã‚£ã‚¢æ²è¼‰å®Ÿç¸¾ã¨ã—ã¦è²´ç¤¾ã‚µã‚¤ãƒˆã«ã¦è¨˜äº‹ã‚’ã”ç´¹ä»‹é ‚ãã“ã¨ã¯å¯èƒ½ã§ã—ã‚‡ã†ã‹ï¼Ÿ

å‚è€ƒã¾ã§ã«ã€ä»–ç¤¾æ§˜ã§ã®æ²è¼‰äº‹ä¾‹ã‚’ä»¥ä¸‹ã«ãŠç¤ºã—ã„ãŸã—ã¾ã™ã€‚
https://evigym.com/news/media-personal-fitness
https://corp.azure-collaboration.co.jp/media-personal-fitness/

ã¾ãŸã€è¨˜äº‹å†…å®¹ã«é–¢ã™ã‚‹ãŠå•ã„åˆã‚ã›ãªã©ã”ã–ã„ã¾ã—ãŸã‚‰ã€ã”å›ç­”ã•ã›ã¦ã„ãŸã ãã¾ã™ãŸã‚ã€ã”é€£çµ¡ã—ã¦ã„ãŸã ã‘ã¾ã™ã¨å¹¸ã„ã§ã™ã€‚

ä¸èº¾ãªã”é€£çµ¡ã¨ãªã‚Šæã‚Œå…¥ã‚Šã¾ã™ãŒã€ä»Šå¾Œã¨ã‚‚personal-fitnessã‚’ä½•å’ã‚ˆã‚ã—ããŠé¡˜ã„ã„ãŸã—ã¾ã™ã€‚"""
        
        result = {
            "success": True,
            "region": region,
            "searched_posts": len(raw_posts),
            "valid_posts": len(valid_posts),
            "total_stores": sum(len(p["store_ids"]) for p in valid_posts),
            "posts": valid_posts,
            "email_template": email_template,
            "anchor_urls": all_anchor_urls
        }
        
        logger.info(f"Generated email template for {len(valid_posts)} posts in {region}")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.exception(f"Error in generate_gym_introduction_email: {e}")
        return json.dumps({
            "success": False,
            "error": str(e),
            "region": region
        }, ensure_ascii=False)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# Google Sheets ãƒ„ãƒ¼ãƒ«
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

def _get_google_sheets_service():
    """
    Google Sheets APIã®ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚
    èªè¨¼æƒ…å ±JSONãƒ•ã‚¡ã‚¤ãƒ«ã¯ä»¥ä¸‹ã®é †åºã§æ¤œç´¢ã•ã‚Œã¾ã™:
    1. ç’°å¢ƒå¤‰æ•° GOOGLE_APPLICATION_CREDENTIALS
    2. server.py ã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    3. ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® mcp-servers/scraping-mcp-server/
    """
    if not GOOGLE_SHEETS_AVAILABLE:
        raise RuntimeError(
            "Google Sheets API ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
            "ä»¥ä¸‹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: google-api-python-client, google-auth"
        )
    
    # èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«å
    creds_filename = "braided-circuit-465415-m6-1cbbf338d9f0.json"
    
    # æ¤œç´¢ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    possible_paths = []
    
    # 1. ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—
    env_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    if env_path:
        possible_paths.append(Path(env_path))
    
    # 2. server.pyã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    possible_paths.append(Path(__file__).parent / creds_filename)
    
    # 3. ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã® mcp-servers/scraping-mcp-server/
    home_dir = Path.home()
    possible_paths.append(home_dir / "mcp-servers" / "scraping-mcp-server" / creds_filename)
    
    # 4. é–‹ç™ºç”¨: Desktop/02_é–‹ç™º/scraping-mcp-server/
    desktop_path = home_dir / "Desktop" / "02_é–‹ç™º" / "scraping-mcp-server" / creds_filename
    possible_paths.append(desktop_path)
    
    # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸãƒ‘ã‚¹ã‚’ä½¿ç”¨
    creds_path = None
    for path in possible_paths:
        if path.exists():
            creds_path = path
            logger.info(f"èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ: {creds_path}")
            break
    
    if not creds_path:
        searched_paths = "\n".join([f"  - {p}" for p in possible_paths])
        raise RuntimeError(
            f"èªè¨¼æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {creds_filename}\n"
            f"ä»¥ä¸‹ã®å ´æ‰€ã‚’ç¢ºèªã—ã¾ã—ãŸ:\n{searched_paths}\n"
            f"ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° GOOGLE_APPLICATION_CREDENTIALS ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚"
        )
    
    try:
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®èªè¨¼æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€
        credentials = service_account.Credentials.from_service_account_file(
            creds_path,
            scopes=['https://www.googleapis.com/auth/spreadsheets.readonly']
        )
        
        # Google Sheets APIã®ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        service = build('sheets', 'v4', credentials=credentials)
        return service
    except Exception as e:
        logger.exception(f"Google Sheets APIèªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        raise RuntimeError(f"Google Sheets APIã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {str(e)}")


@mcp.tool()
async def read_google_sheet(
    spreadsheet_id: str,
    range_name: str = "",
    sheet_name: str = ""
) -> str:
    """
    Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚
    
    Args:
        spreadsheet_id: ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDï¼ˆURLã® /d/ ã¨ /edit ã®é–“ã®æ–‡å­—åˆ—ï¼‰
            ä¾‹: "1BxiMVs0XRA5nFMdKvBdBZjgmUUqptlbs74OgvE2upms"
        range_name: èª­ã¿è¾¼ã‚€ç¯„å›²ï¼ˆä¾‹: "A1:C10", "Sheet1!A1:Z100"ï¼‰
            ç©ºæ–‡å­—åˆ—ã®å ´åˆã¯ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’èª­ã¿è¾¼ã¿ã¾ã™
        sheet_name: ã‚·ãƒ¼ãƒˆåï¼ˆrange_nameã«ã‚·ãƒ¼ãƒˆåãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã«ä½¿ç”¨ï¼‰
            ä¾‹: "Sheet1"
    
    Returns:
        ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONå½¢å¼ã§è¿”ã—ã¾ã™
    """
    logger.info(
        f"read_google_sheet called with spreadsheet_id={spreadsheet_id}, "
        f"range_name={range_name}, sheet_name={sheet_name}"
    )
    
    if not GOOGLE_SHEETS_AVAILABLE:
        return json.dumps({
            "success": False,
            "error": "google_sheets_unavailable",
            "message": (
                "Google Sheets API ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚"
                "ä»¥ä¸‹ã®ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: "
                "google-api-python-client, google-auth"
            )
        }, ensure_ascii=False)
    
    try:
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        service = _get_google_sheets_service()
        
        # ç¯„å›²ã‚’æ§‹ç¯‰
        if range_name:
            # range_nameã«ã‚·ãƒ¼ãƒˆåãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            if '!' in range_name:
                full_range = range_name
            elif sheet_name:
                full_range = f"{sheet_name}!{range_name}"
            else:
                full_range = range_name
        elif sheet_name:
            full_range = sheet_name
        else:
            # ç¯„å›²ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æœ€åˆã®ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’èª­ã¿è¾¼ã‚€
            full_range = None
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆã‚·ãƒ¼ãƒˆåã®ç¢ºèªç”¨ï¼‰
        spreadsheet_metadata = service.spreadsheets().get(
            spreadsheetId=spreadsheet_id
        ).execute()
        
        sheet_names = [sheet['properties']['title'] for sheet in spreadsheet_metadata.get('sheets', [])]
        logger.info(f"Available sheets: {sheet_names}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€
        if full_range:
            result = service.spreadsheets().values().get(
                spreadsheetId=spreadsheet_id,
                range=full_range
            ).execute()
        else:
            # ç¯„å›²ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€æœ€åˆã®ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’èª­ã¿è¾¼ã‚€
            if sheet_names:
                first_sheet = sheet_names[0]
                result = service.spreadsheets().values().get(
                    spreadsheetId=spreadsheet_id,
                    range=first_sheet
                ).execute()
            else:
                return json.dumps({
                    "success": False,
                    "error": "no_sheets",
                    "message": "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                }, ensure_ascii=False)
        
        values = result.get('values', [])
        
        if not values:
            return json.dumps({
                "success": True,
                "spreadsheet_id": spreadsheet_id,
                "range": full_range or sheet_names[0] if sheet_names else "unknown",
                "row_count": 0,
                "data": [],
                "message": "æŒ‡å®šã•ã‚ŒãŸç¯„å›²ã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
            }, ensure_ascii=False, indent=2)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢
        # æœ€åˆã®è¡Œã‚’ãƒ˜ãƒƒãƒ€ãƒ¼ã¨ã—ã¦æ‰±ã†
        headers = values[0] if values else []
        rows = values[1:] if len(values) > 1 else []
        
        # è¾æ›¸å½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
        data_rows = []
        for row in rows:
            row_dict = {}
            for i, header in enumerate(headers):
                row_dict[header] = row[i] if i < len(row) else ""
            data_rows.append(row_dict)
        
        result_data = {
            "success": True,
            "spreadsheet_id": spreadsheet_id,
            "range": full_range or sheet_names[0] if sheet_names else "unknown",
            "sheet_names": sheet_names,
            "row_count": len(values),
            "header_count": len(headers),
            "headers": headers,
            "data": data_rows,
            "raw_values": values  # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚‚å«ã‚ã‚‹
        }
        
        logger.info(f"Successfully read {len(values)} rows from spreadsheet")
        return json.dumps(result_data, ensure_ascii=False, indent=2)
        
    except HttpError as e:
        error_details = json.loads(e.content.decode('utf-8'))
        logger.error(f"Google Sheets API HTTPã‚¨ãƒ©ãƒ¼: {error_details}")
        return json.dumps({
            "success": False,
            "error": "api_error",
            "status_code": e.resp.status,
            "message": error_details.get('error', {}).get('message', str(e)),
            "details": error_details
        }, ensure_ascii=False, indent=2)
    except RuntimeError as e:
        logger.error(f"Google Sheets èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return json.dumps({
            "success": False,
            "error": "authentication_error",
            "message": str(e)
        }, ensure_ascii=False)
    except Exception as e:
        logger.exception(f"Error in read_google_sheet: {e}")
        return json.dumps({
            "success": False,
            "error": "unexpected_error",
            "message": f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        }, ensure_ascii=False)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def main():
    """MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    mcp.run(transport="stdio")

if __name__ == "__main__":
    main()
