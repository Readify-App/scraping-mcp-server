# server.py
# Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”¨MCPã‚µãƒ¼ãƒãƒ¼

import logging
import asyncio
import json
from urllib.parse import urljoin, urlparse
from typing import List
from bs4 import BeautifulSoup
from bs4.element import Tag
from mcp.server.fastmcp import FastMCP
import aiohttp
from aiohttp import ClientTimeout

# Playwrightã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    async_playwright = None
    PLAYWRIGHT_AVAILABLE = False

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/Users/yuta/Desktop/02_é–‹ç™º/scraping-mcp-server/debug.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
MAX_BROWSERS = 5
browser_semaphore = asyncio.Semaphore(MAX_BROWSERS)

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# MCPã‚µãƒ¼ãƒãƒ¼ä½œæˆ
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
mcp = FastMCP("web-scraping-server")

# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ãƒ„ãƒ¼ãƒ«å®šç¾©
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

@mcp.tool()
async def fetch_page_content(url: str) -> str:
    """
    æŒ‡å®šURLã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€ãƒ¡ã‚¤ãƒ³éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    é€šå¸¸ã®HTMLãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URLï¼ˆä¾‹ï¼šã€Œhttps://example.com/pageã€ï¼‰
    
    Returns:
        ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆJSONå½¢å¼ï¼‰
    """
    logger.info(f"fetch_page_content called with url={url}")
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=ClientTimeout(total=20)) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch page: {response.status}"
                    logger.error(error_msg)
                    return f"ã‚¨ãƒ©ãƒ¼: {error_msg}\nURL: {url}"
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ä¸è¦ãªã‚¿ã‚°ã‚’é™¤å»
                for tag in soup(["script", "style", "noscript", "header", "footer", "nav"]):
                    tag.decompose()
                
                # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™ï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
                main_content = None
                
                # 1. main ã‚¿ã‚°
                main_content = soup.find("main")
                
                # 2. article ã‚¿ã‚°
                if not main_content:
                    main_content = soup.find("article")
                
                # 3. role="main" å±æ€§
                if not main_content:
                    main_content = soup.find(attrs={"role": "main"})
                
                # 4. id or class ã« content/main ãŒå«ã¾ã‚Œã‚‹è¦ç´ 
                if not main_content:
                    for selector in ["#content", ".content", "#main", ".main", 
                                   "[id*='content']", "[class*='content']"]:
                        main_content = soup.select_one(selector)
                        if main_content:
                            break
                
                # 5. bodyå…¨ä½“ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if not main_content:
                    main_content = soup.find("body")
                
                # ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                if main_content:
                    # æ”¹è¡Œã‚„ã‚¿ãƒ–ã‚’æ­£è¦åŒ–
                    text = main_content.get_text(separator="\n", strip=True)
                    # é€£ç¶šã™ã‚‹ç©ºç™½è¡Œã‚’å‰Šé™¤
                    lines = [line.strip() for line in text.split('\n') if line.strip()]
                    content = '\n'.join(lines)
                else:
                    content = ""
                
                # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å–å¾—ï¼ˆå‚è€ƒæƒ…å ±ã¨ã—ã¦ï¼‰
                title = ""
                title_tag = soup.find("title")
                if title_tag:
                    title = title_tag.get_text(strip=True)
                
                logger.info(f"Successfully extracted content: {len(content)} chars")
                
                # çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™
                result = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                result += f"ğŸ“„ {title}\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                result += f"ğŸ”— URL: {url}\n"
                result += f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(content)} æ–‡å­—\n\n"
                result += f"ã€æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘\n\n"
                result += content
                
                return result
                
    except Exception as e:
        logger.exception(f"Error in fetch_page_content: {e}")
        return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nURL: {url}"


@mcp.tool()
async def fetch_page_content_with_playwright(url: str) -> str:
    """
    Playwrightã‚’ä½¿ç”¨ã—ã¦JavaScript/SPA/Reactã‚µã‚¤ãƒˆã®ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã¾ã™ã€‚
    å‹•çš„ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    Shadow DOMã‚„ã‚«ã‚¹ã‚¿ãƒ è¦ç´ ã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚
    
    Args:
        url: å–å¾—å¯¾è±¡ã®URLï¼ˆä¾‹ï¼šã€Œhttps://example.com/spa-pageã€ï¼‰
    
    Returns:
        ãƒšãƒ¼ã‚¸ã®ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆJSONå½¢å¼ï¼‰
    """
    logger.info(f"fetch_page_content_with_playwright called with url={url}")
    
    if not PLAYWRIGHT_AVAILABLE:
        error_msg = "Playwright is not available. Please install it with: pip install playwright && playwright install"
        logger.error(error_msg)
        return f"ã‚¨ãƒ©ãƒ¼: {error_msg}"
    
    if not async_playwright:
        return "ã‚¨ãƒ©ãƒ¼: Playwright not available"
    
    # PDFã‚’äº‹å‰ã«é™¤å¤–
    if url.lower().endswith('.pdf'):
        logger.warning(f"Skipping PDF: {url}")
        return f"ã‚¨ãƒ©ãƒ¼: PDFãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“\nURL: {url}"
    
    async with browser_semaphore:  # ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
        browser = None
        context = None
        page = None
        try:
            logger.debug(f"Starting Playwright extraction for: {url}")
            async with async_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--single-process']
                )
                # Contextã‚’ä½œæˆ
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                # User-Agentã‚’è¨­å®š
                await page.set_extra_http_headers({
                    "User-Agent": (
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    ),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                })
                
                # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                logger.debug(f"Navigating to {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=15000)
                
                # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒè½ã¡ç€ãã¾ã§å¾…æ©Ÿ
                await page.wait_for_load_state("networkidle")
                
                # ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼åŒæ„ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®å‡¦ç†
                try:
                    consent_selectors = [
                        "#accept",
                        ".accept.uc-accept-button",
                        "[data-action='consent'][data-action-type='accept']",
                        "button:has-text('ã™ã¹ã¦å—ã‘å…¥ã‚Œã‚‹')",
                        "button:has-text('åŒæ„')",
                        "button:has-text('Accept')",
                        "button:has-text('OK')",
                    ]
                    
                    for selector in consent_selectors:
                        try:
                            if await page.locator(selector).count() > 0:
                                await page.click(selector, timeout=2000)
                                await page.wait_for_timeout(1000)
                                break
                        except:
                            continue
                except:
                    pass
                
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã®ã‚’å¾…ã¤
                try:
                    await page.wait_for_selector(
                        "main, article, [role='main'], .content, #content, .main-content",
                        timeout=10000
                    )
                except:
                    logger.warning("No main content selector found, continuing...")
                
                # SPAã®ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã‚’å¾…ã¤
                await page.wait_for_timeout(5000)
                
                # JavaScriptã§ç›´æ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ï¼ˆShadow DOMå¯¾å¿œï¼‰
                content_data = await page.evaluate("""
                    () => {
                        // Shadow DOMã‚‚å«ã‚ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã™ã‚‹é–¢æ•°
                        function extractAllText(element) {
                            let text = '';
                            
                            // Shadow rootãŒã‚ã‚‹å ´åˆ
                            if (element.shadowRoot) {
                                const shadowElements = element.shadowRoot.querySelectorAll('*');
                                shadowElements.forEach(el => {
                                    text += extractAllText(el) + ' ';
                                });
                            }
                            
                            // é€šå¸¸ã®DOMè¦ç´ ã®å‡¦ç†
                            if (element.nodeType === Node.TEXT_NODE) {
                                text += element.textContent || '';
                            } else if (element.nodeType === Node.ELEMENT_NODE) {
                                // ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¯é™¤å¤–
                                if (!['SCRIPT', 'STYLE', 'NOSCRIPT', 'META', 'LINK'].includes(element.tagName)) {
                                    for (const child of element.childNodes) {
                                        text += extractAllText(child) + ' ';
                                    }
                                }
                            }
                            
                            return text;
                        }
                        
                        // äº‹å‰ã«scriptã¨styleã‚’å…¨ã¦å‰Šé™¤
                        document.querySelectorAll('script, style, noscript').forEach(el => el.remove());
                        
                        // ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
                        const selectors = [
                            'main',
                            'article',
                            '[role="main"]',
                            '#content',
                            '.content',
                            '#main',
                            '.main',
                            '.main-content',
                            '.page-content',
                            'body'
                        ];
                        
                        let mainContent = null;
                        for (const selector of selectors) {
                            mainContent = document.querySelector(selector);
                            if (mainContent) break;
                        }
                        
                        // ãƒ˜ãƒƒãƒ€ãƒ¼ã€ãƒ•ãƒƒã‚¿ãƒ¼ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é™¤å¤–
                        if (mainContent) {
                            const excludeSelectors = ['header', 'footer', 'nav', '.header', '.footer', '.navigation'];
                            excludeSelectors.forEach(selector => {
                                const elements = mainContent.querySelectorAll(selector);
                                elements.forEach(el => el.remove());
                            });
                            
                            mainContent.querySelectorAll('script, style, noscript').forEach(el => el.remove());
                        }
                        
                        // Shadow DOMã‚‚å«ã‚ãŸå…¨ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
                        const shadowDomText = mainContent ? extractAllText(mainContent) : '';
                        const innerText = mainContent ? mainContent.innerText : '';
                        const textContent = mainContent ? mainContent.textContent : '';
                        
                        // ã‚¿ã‚¤ãƒˆãƒ«ã‚‚å–å¾—
                        const title = document.title || '';
                        
                        return {
                            title: title,
                            shadowDomText: shadowDomText.trim(),
                            innerText: innerText.trim(),
                            textContent: textContent.trim(),
                            shadowDomLength: shadowDomText.length,
                            innerTextLength: innerText.length,
                            textContentLength: textContent.length
                        };
                    }
                """)
                
                # mailto: ã¨ tel: ã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                try:
                    link_hrefs = await page.evaluate("""
                        () => Array.from(document.querySelectorAll('a[href^="mailto:"], a[href^="tel:"]'))
                            .map(a => a.getAttribute('href') || '')
                    """)
                except Exception:
                    link_hrefs = []
                
                # æœ€é©ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’é¸æŠ
                content = ""
                
                # Nuxt.js/Vue.jsã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
                def is_json_data(text):
                    if not text:
                        return False
                    json_patterns = ['window.__NUXT__', '["[\"Reactive\"', '{"data":', 'googleapis.com']
                    return any(pattern in text[:500] for pattern in json_patterns)
                
                if content_data.get('innerText') and len(content_data['innerText']) > 100 and not is_json_data(content_data['innerText']):
                    content = content_data['innerText']
                    logger.debug(f"Using innerText: {len(content)} chars")
                elif content_data.get('shadowDomText') and len(content_data['shadowDomText']) > 100 and not is_json_data(content_data['shadowDomText']):
                    content = content_data['shadowDomText']
                    logger.debug(f"Using shadowDomText: {len(content)} chars")
                elif content_data.get('textContent') and not is_json_data(content_data['textContent']):
                    content = content_data['textContent']
                    logger.debug(f"Using textContent: {len(content)} chars")
                else:
                    content = ""
                    logger.warning("No valid content found (JSON data detected)")
                
                # ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–
                if content:
                    content = ' '.join(content.split())
                    lines = content.split('.')
                    content = '.\n'.join(line.strip() for line in lines if line.strip())
                
                # æŠ½å‡ºã—ãŸãƒ¡ãƒ¼ãƒ«ãƒ»é›»è©±ç•ªå·ã‚’æ•´å½¢
                extracted_emails = []
                extracted_phones = []
                try:
                    for href in link_hrefs:
                        h = (href or '').strip()
                        if h.lower().startswith('mailto:'):
                            email = h.split(':', 1)[1].split('?', 1)[0]
                            if email:
                                extracted_emails.append(email)
                        elif h.lower().startswith('tel:'):
                            number = h.split(':', 1)[1]
                            if number:
                                extracted_phones.append(number)
                except Exception:
                    pass
                
                logger.info(f"Successfully extracted content: {len(content)} chars")
                
                # çµæœã‚’æ•´å½¢ã—ã¦è¿”ã™
                result = f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n"
                result += f"ğŸ“„ {content_data.get('title', '')}\n"
                result += f"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\n"
                result += f"ğŸ”— URL: {url}\n"
                result += f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·: {len(content)} æ–‡å­—\n"
                result += f"ğŸ”§ æŠ½å‡ºæ–¹æ³•: Playwright (JavaScript rendering)\n"
                
                if extracted_emails:
                    result += f"ğŸ“§ ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: {', '.join(set(extracted_emails))}\n"
                if extracted_phones:
                    result += f"ğŸ“ é›»è©±ç•ªå·: {', '.join(set(extracted_phones))}\n"
                
                result += f"\nã€æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã€‘\n\n"
                result += content
                
                if len(content) < 100:
                    result += "\n\nâš ï¸ è­¦å‘Š: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒå°‘ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚èªè¨¼ãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã‹ã€å‹•çš„èª­ã¿è¾¼ã¿ã«æ™‚é–“ãŒã‹ã‹ã‚‹ãƒšãƒ¼ã‚¸ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"
                
                return result
                
        except Exception as e:
            logger.exception(f"Error in fetch_page_content_with_playwright: {e}")
            return f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\nURL: {url}"
        finally:
            # å®Œå…¨ãªçµ‚äº†å‡¦ç†
            try:
                if page:
                    await page.close()
            except:
                pass
            try:
                if context:
                    await context.close()
            except:
                pass
            try:
                if browser:
                    await browser.close()
                    await asyncio.sleep(1)
            except:
                pass


@mcp.tool()
async def extract_site_links(url: str) -> str:
    """
    å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰header/footer/navã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã€ä»®æƒ³ã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆã—ã¾ã™ã€‚
    é€šå¸¸ã®HTMLãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å¯¾è±¡ã‚µã‚¤ãƒˆã®URLï¼ˆä¾‹ï¼šã€Œhttps://www.goldsgym.jpã€ï¼‰
    
    Returns:
        ã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯æƒ…å ±ã®JSONæ–‡å­—åˆ—ã€‚å„ãƒªãƒ³ã‚¯ã«ã¯text, urlã€content_headings(è¦‹å‡ºã—ãƒªã‚¹ãƒˆ)ãŒå«ã¾ã‚Œã‚‹
    """
    logger.info(f"extract_site_links called with url={url}")
    
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Cache-Control": "max-age=0"
    }
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=ClientTimeout(total=20)) as response:
                if response.status != 200:
                    error_msg = f"Failed to fetch page: {response.status}"
                    logger.error(error_msg)
                    return json.dumps({
                        "error": error_msg,
                        "base_url": url,
                        "links": []
                    }, ensure_ascii=False)
                
                html = await response.text()
                soup = BeautifulSoup(html, "html.parser")
                
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
                for tag in soup(["script", "style", "noscript"]):
                    tag.decompose()
                
                # header/footer/navè¦ç´ ã‚’æ¢ã™
                header = soup.find("header") or soup.select_one('[role="banner"]')
                footer = soup.find("footer") or soup.select_one('[role="contentinfo"]')
                
                # ç‹¬ç«‹ã—ãŸnavè¦ç´ ã‚’æ¢ã™ï¼ˆheaderã®å¤–ã«ã‚ã‚‹ã‚‚ã®ï¼‰
                nav_elements = soup.find_all("nav")
                independent_navs = []
                if header and isinstance(header, Tag):
                    header_navs = header.find_all("nav")
                    independent_navs = [nav for nav in nav_elements if nav not in header_navs]
                else:
                    independent_navs = nav_elements
                
                # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹å†…éƒ¨é–¢æ•°
                def extract_links_from_element(element):
                    if element is None:
                        return []
                    
                    links = []
                    for link in element.find_all('a', href=True):
                        text = link.get_text(strip=True)
                        href = link['href']
                        
                        # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                        absolute_url = urljoin(url, href)
                        
                        # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿å‡¦ç†
                        if text:
                            links.append({
                                'text': text,
                                'url': absolute_url
                            })
                    return links
                
                # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                header_links = extract_links_from_element(header)
                footer_links = extract_links_from_element(footer)
                nav_links = []
                for nav in independent_navs:
                    nav_links.extend(extract_links_from_element(nav))
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã¨é™¤å»
                def extract_url_pattern(url: str, base_url: str = "") -> str:
                    """URLã‹ã‚‰ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                    try:
                        parsed = urlparse(url)
                        base_parsed = urlparse(base_url)
                        
                        # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹éƒ¨åˆ†ã‚’é™¤å¤–
                        base_path = base_parsed.path.strip('/')
                        full_path = parsed.path.strip('/')
                        
                        # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é™¤å¤–
                        if base_path and full_path.startswith(base_path):
                            relative_path = full_path[len(base_path):].strip('/')
                        else:
                            relative_path = full_path
                        
                        if relative_path:
                            parts = relative_path.split('/')
                            if len(parts) >= 2:
                                pattern_parts = parts[:-1] + ['*']
                                return '/' + '/'.join(pattern_parts) + '/'
                        
                        return parsed.path
                    except:
                        return url
                
                def detect_repeated_patterns(all_links: list, threshold: int = 10, base_url: str = "") -> set:
                    """åŒã˜ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ãŒé–¾å€¤å›æ•°ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                    pattern_counts = {}
                    url_to_pattern = {}
                    
                    for link in all_links:
                        url = link['url']
                        pattern = extract_url_pattern(url, base_url)
                        
                        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                        url_to_pattern[url] = pattern
                    
                    repeated_patterns = {pattern for pattern, count in pattern_counts.items() 
                                       if count >= threshold}
                    
                    urls_to_exclude = set()
                    for url, pattern in url_to_pattern.items():
                        if pattern in repeated_patterns:
                            urls_to_exclude.add(url)
                    
                    return urls_to_exclude
                
                # å…¨ãƒªãƒ³ã‚¯ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
                all_links = []
                seen_urls = set()
                
                for link_list in [header_links, footer_links, nav_links]:
                    for link in link_list:
                        url_key = link['url']
                        if url_key not in seen_urls:
                            seen_urls.add(url_key)
                            all_links.append(link)
                
                # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦é™¤å»ï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰
                urls_to_exclude = detect_repeated_patterns(all_links, threshold=10, base_url=url)
                filtered_links = [link for link in all_links 
                                if link['url'] not in urls_to_exclude]

                # è¦‹å‡ºã—æŠ½å‡ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆh2/h3 ã‚’çµ±åˆã—ãŸå˜ä¸€ãƒªã‚¹ãƒˆï¼‰
                def extract_headings(soup: BeautifulSoup) -> List[str]:
                    h2_list = [h.get_text(strip=True) for h in soup.find_all('h2') if h.get_text(strip=True)]
                    h3_list = [h.get_text(strip=True) for h in soup.find_all('h3') if h.get_text(strip=True)]
                    merged = h2_list + h3_list
                    # é‡è¤‡é™¤å»ã‚’ä¿æŒé †ã§è¡Œã†
                    seen = set()
                    unique_list: List[str] = []
                    for item in merged:
                        if item not in seen:
                            seen.add(item)
                            unique_list.append(item)
                    return unique_list[:100]

                async def fetch_headings_for_url(session: aiohttp.ClientSession, target_url: str) -> List[str]:
                    try:
                        async with session.get(target_url, headers=headers, timeout=ClientTimeout(total=15)) as resp:
                            if resp.status != 200:
                                return []
                            html_text = await resp.text()
                            page_soup = BeautifulSoup(html_text, "html.parser")
                            # ãƒã‚¤ã‚ºã«ãªã‚ŠãŒã¡ãªè¦ç´ ã¯è½ã¨ã™
                            for t in page_soup(["script", "style", "noscript"]):
                                t.decompose()
                            return extract_headings(page_soup)
                    except Exception:
                        return []

                # åŒä¸€ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é™å®šã—ã¦ä¸¦è¡Œã§è¦‹å‡ºã—ã‚’å–å¾—
                base_domain = urlparse(url).netloc.split(':')[0].lower()
                def is_same_domain(target: str) -> bool:
                    try:
                        netloc = urlparse(target).netloc.split(':')[0].lower()
                        # ã‚µãƒ–ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚‚è¨±å¯ï¼ˆexample.com ã¨ www.example.com ãªã©ï¼‰
                        return netloc == base_domain or netloc.endswith('.' + base_domain)
                    except Exception:
                        return False

                max_fetch = 20
                concurrency = 5
                sem = asyncio.Semaphore(concurrency)

                eligible_indices = [i for i, l in enumerate(filtered_links) if is_same_domain(l['url'])][:max_fetch]

                async def bound_fetch(idx: int, target_url: str):
                    async with sem:
                        return idx, await fetch_headings_for_url(session, target_url)

                tasks = [asyncio.create_task(bound_fetch(i, filtered_links[i]['url'])) for i in eligible_indices]
                if tasks:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    for res in results:
                        if isinstance(res, tuple):
                            idx, headings = res
                            filtered_links[idx]["content_headings"] = headings
                # æœªä»˜ä¸ã®ãƒªãƒ³ã‚¯ã«ã¯ç©ºé…åˆ—ã‚’è¨­å®š
                for link_item in filtered_links:
                    if "content_headings" not in link_item:
                        link_item["content_headings"] = []
                
                logger.info(f"Successfully extracted {len(filtered_links)} links from {url}")
                
                # çµæœã‚’è¿”ã™
                return json.dumps({
                    "base_url": url,
                    "total_links": len(all_links),
                    "filtered_links": len(filtered_links),
                    "links": filtered_links,
                    "sections": {
                        "header_links": len(header_links),
                        "footer_links": len(footer_links),
                        "nav_links": len(nav_links)
                    }
                }, ensure_ascii=False)
                
    except Exception as e:
        logger.exception(f"Error in extract_site_links: {e}")
        return json.dumps({
            "error": str(e),
            "base_url": url,
            "links": []
        }, ensure_ascii=False)


@mcp.tool()
async def extract_site_links_with_playwright(url: str) -> str:
    """
    Playwrightã‚’ä½¿ç”¨ã—ã¦JavaScript/SPA/Reactã‚µã‚¤ãƒˆã‹ã‚‰header/footer/navã®ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã™ã€‚
    å‹•çš„ã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ãƒšãƒ¼ã‚¸ã«æœ€é©ã§ã™ã€‚
    
    Args:
        url: å¯¾è±¡ã‚µã‚¤ãƒˆã®URLï¼ˆä¾‹ï¼šã€Œhttps://www.goldsgym.jpã€ï¼‰
    
    Returns:
        ã‚µã‚¤ãƒˆã®ãƒªãƒ³ã‚¯æƒ…å ±ã®JSONæ–‡å­—åˆ—ã€‚å„ãƒªãƒ³ã‚¯ã«ã¯text, urlã€content_headings(è¦‹å‡ºã—ãƒªã‚¹ãƒˆ)ãŒå«ã¾ã‚Œã‚‹
    """
    logger.info(f"extract_site_links_with_playwright called with url={url}")
    
    if not PLAYWRIGHT_AVAILABLE:
        error_msg = "Playwright is not available. Please install it with: pip install playwright && playwright install"
        logger.error(error_msg)
        return json.dumps({
            "error": error_msg,
            "base_url": url,
            "links": []
        }, ensure_ascii=False)
    
    if not async_playwright:
        return json.dumps({"error": "Playwright not available", "base_url": url, "links": []}, ensure_ascii=False)
    
    # PDFã‚’äº‹å‰ã«é™¤å¤–
    if url.lower().endswith('.pdf'):
        logger.warning(f"Skipping PDF: {url}")
        return json.dumps({"error": "PDF files are not supported", "base_url": url, "links": []}, ensure_ascii=False)
    
    async with browser_semaphore:  # ãƒ–ãƒ©ã‚¦ã‚¶æ•°åˆ¶é™
        browser = None
        context = None
        page = None
        try:
            logger.debug(f"Starting Playwright link extraction for: {url}")
            async with async_playwright() as p:
                # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’èµ·å‹•
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--single-process']
                )
                # Contextã‚’ä½œæˆ
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080}
                )
                page = await context.new_page()
                
                # User-Agentã‚’è¨­å®š
                await page.set_extra_http_headers({
                    "User-Agent": (
                        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    ),
                    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                    "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
                    "Accept-Encoding": "gzip, deflate",
                    "DNT": "1",
                    "Connection": "keep-alive",
                    "Upgrade-Insecure-Requests": "1",
                    "Sec-Fetch-Dest": "document",
                    "Sec-Fetch-Mode": "navigate",
                    "Sec-Fetch-Site": "none",
                    "Cache-Control": "max-age=0"
                })
                
                # ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
                logger.debug(f"Navigating to {url}")
                await page.goto(url, wait_until="domcontentloaded", timeout=15000)
            
                # SPAã®åˆæœŸãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å¾…ã¤
                logger.debug("Waiting for network idle")
                await page.wait_for_load_state("networkidle")
            
                # å‹•çš„è¦ç´ ã®å‡ºç¾ã‚’å¾…ã¤
                try:
                    await page.wait_for_selector(
                        "header, nav, footer, [role='navigation'], .header, .navbar, .navigation",
                        timeout=10000
                    )
                    logger.debug("Navigation elements found")
                except:
                    logger.warning(f"No navigation elements found immediately for {url}")
                
                # è¿½åŠ ã®å¾…æ©Ÿï¼ˆå‹•çš„ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°å®Œäº†ã®ãŸã‚ï¼‰
                await page.wait_for_timeout(5000)
                
                # JavaScriptã§ãƒªãƒ³ã‚¯æ•°ã‚’äº‹å‰ç¢ºèªï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
                js_link_info = await page.evaluate("""
                    () => {
                        return {
                            total: document.querySelectorAll('a').length,
                            inHeader: document.querySelectorAll('header a, .header a').length,
                            inNav: document.querySelectorAll('nav a, .nav a, .navbar a').length,
                            inFooter: document.querySelectorAll('footer a, .footer a').length,
                            hasHeader: !!document.querySelector('header, .header'),
                            hasNav: !!document.querySelector('nav, .nav, .navbar'),
                            hasFooter: !!document.querySelector('footer, .footer')
                        }
                    }
                """)
                logger.debug(f"Link info: {js_link_info}")
                
                # HTMLã‚’å–å¾—
                html = await page.content()
            
            # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(html, "html.parser")
            
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
            for tag in soup(["script", "style", "noscript"]):
                tag.decompose()
            
            # header/footer/navè¦ç´ ã‚’æ¢ã™ï¼ˆã‚»ãƒ¬ã‚¯ã‚¿ã‚’æ‹¡å¼µï¼‰
            header = (soup.find("header") or 
                     soup.select_one('[role="banner"]') or
                     soup.select_one('.header') or
                     soup.select_one('#header'))
            
            footer = (soup.find("footer") or 
                     soup.select_one('[role="contentinfo"]') or
                     soup.select_one('.footer') or
                     soup.select_one('#footer'))
            
            # ç‹¬ç«‹ã—ãŸnavè¦ç´ ã‚’æ¢ã™ï¼ˆheaderã®å¤–ã«ã‚ã‚‹ã‚‚ã®ï¼‰
            nav_elements = soup.find_all("nav")
            nav_elements.extend(soup.select('.nav, .navbar, .navigation'))
            
            independent_navs = []
            if header and isinstance(header, Tag):
                header_navs = header.find_all("nav")
                header_navs.extend(header.select('.nav, .navbar, .navigation'))
                independent_navs = [nav for nav in nav_elements if nav not in header_navs]
            else:
                independent_navs = nav_elements
            
            # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹å†…éƒ¨é–¢æ•°
            def extract_links_from_element(element):
                if element is None:
                    return []
                
                links = []
                for link in element.find_all('a', href=True):
                    text = link.get_text(strip=True)
                    href = link['href']
                    
                    # tel:, mailto:, javascript: ãªã©ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if href.startswith(('tel:', 'mailto:', 'javascript:', '#')):
                        continue
                    
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    absolute_url = urljoin(url, href)
                    
                    # ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ãªã„å ´åˆã®ã¿å‡¦ç†
                    if text:
                        links.append({
                            'text': text,
                            'url': absolute_url
                        })
                return links
            
            # å„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
            header_links = extract_links_from_element(header)
            footer_links = extract_links_from_element(footer)
            nav_links = []
            for nav in independent_navs:
                nav_links.extend(extract_links_from_element(nav))
            
            logger.debug(f"Extracted - Header: {len(header_links)}, Footer: {len(footer_links)}, Nav: {len(nav_links)}")
            
            # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡ºã¨é™¤å»ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ç¶­æŒï¼‰
            def extract_url_pattern(url: str, base_url: str = "") -> str:
                """URLã‹ã‚‰ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                try:
                    parsed = urlparse(url)
                    base_parsed = urlparse(base_url)
                    
                    # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹éƒ¨åˆ†ã‚’é™¤å¤–
                    base_path = base_parsed.path.strip('/')
                    full_path = parsed.path.strip('/')
                    
                    # ãƒ™ãƒ¼ã‚¹URLã®ãƒ‘ã‚¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã‚Œã‚’é™¤å¤–
                    if base_path and full_path.startswith(base_path):
                        relative_path = full_path[len(base_path):].strip('/')
                    else:
                        relative_path = full_path
                    
                    if relative_path:
                        parts = relative_path.split('/')
                        if len(parts) >= 2:
                            pattern_parts = parts[:-1] + ['*']
                            return '/' + '/'.join(pattern_parts) + '/'
                    
                    return parsed.path
                except:
                    return url
            
            def detect_repeated_patterns(all_links: list, threshold: int = 10, base_url: str = "") -> set:
                """åŒã˜ãƒ‘ãƒ¼ãƒãƒªãƒ³ã‚¯æ§‹é€ ãŒé–¾å€¤å›æ•°ä»¥ä¸Šç¹°ã‚Šè¿”ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰"""
                pattern_counts = {}
                url_to_pattern = {}
                
                for link in all_links:
                    url = link['url']
                    pattern = extract_url_pattern(url, base_url)
                    
                    pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
                    url_to_pattern[url] = pattern
                
                repeated_patterns = {pattern for pattern, count in pattern_counts.items() 
                                   if count >= threshold}
                
                urls_to_exclude = set()
                for url, pattern in url_to_pattern.items():
                    if pattern in repeated_patterns:
                        urls_to_exclude.add(url)
                
                return urls_to_exclude
            
            # å…¨ãƒªãƒ³ã‚¯ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’å‰Šé™¤
            all_links = []
            seen_urls = set()
            
            for link_list in [header_links, footer_links, nav_links]:
                for link in link_list:
                    url_key = link['url']
                    if url_key not in seen_urls:
                        seen_urls.add(url_key)
                        all_links.append(link)
            
            # ãƒªãƒ³ã‚¯ãŒ0ä»¶ã®å ´åˆã®è¿½åŠ å‡¦ç†
            if len(all_links) == 0:
                logger.warning("No links extracted from BeautifulSoup. Trying JavaScript extraction...")
                
                # JavaScriptã§ç›´æ¥ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                if async_playwright:
                    async with async_playwright() as p2:
                        browser2 = await p2.chromium.launch(headless=True, args=['--no-sandbox', '--single-process'])
                        context2 = await browser2.new_context()
                        page2 = await context2.new_page()
                    
                        await page2.goto(url, wait_until="networkidle", timeout=30000)
                        await page2.wait_for_timeout(5000)
                        
                        js_links = await page2.evaluate("""
                        () => {
                            const foundLinks = [];
                            const seen = new Set();
                            
                            document.querySelectorAll('a').forEach(link => {
                                if (link.href && 
                                    !link.href.startsWith('tel:') && 
                                    !link.href.startsWith('mailto:') &&
                                    !link.href.startsWith('javascript:') &&
                                    !seen.has(link.href)) {
                                    seen.add(link.href);
                                    foundLinks.push({
                                        text: link.innerText.trim() || link.textContent.trim() || 'No text',
                                        url: link.href
                                    });
                                }
                            });
                            
                            return foundLinks;
                        }
                        """)
                        
                        await page2.close()
                        await context2.close()
                        await browser2.close()
                        
                        all_links = js_links
                        logger.info(f"JavaScript extraction found {len(all_links)} links")
            
            # é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡ºã—ã¦é™¤å»ï¼ˆãƒ™ãƒ¼ã‚¹URLã‚’è€ƒæ…®ï¼‰
            urls_to_exclude = detect_repeated_patterns(all_links, threshold=10, base_url=url)
            filtered_links = [link for link in all_links 
                            if link['url'] not in urls_to_exclude]
            
            # è¦‹å‡ºã—æŠ½å‡ºã¯ç°¡æ˜“ç‰ˆï¼ˆæ™‚é–“çŸ­ç¸®ã®ãŸã‚ï¼‰
            for link_item in filtered_links:
                link_item["content_headings"] = []
            
            logger.info(f"Successfully extracted {len(filtered_links)} links from {url}")
            
            # çµæœã‚’è¿”ã™
            return json.dumps({
                "base_url": url,
                "total_links": len(all_links),
                "filtered_links": len(filtered_links),
                "links": filtered_links,
                "sections": {
                    "header_links": len(header_links),
                    "footer_links": len(footer_links),
                    "nav_links": len(nav_links)
                },
                "debug_info": js_link_info if 'js_link_info' in locals() else {}
            }, ensure_ascii=False)
                
        except Exception as e:
            logger.exception(f"Error in extract_site_links_with_playwright: {e}")
            return json.dumps({
                "error": str(e),
                "base_url": url,
                "links": []
            }, ensure_ascii=False)
        finally:
            # å®Œå…¨ãªçµ‚äº†å‡¦ç†
            try:
                if page:
                    await page.close()
            except:
                pass
            try:
                if context:
                    await context.close()
            except:
                pass
            try:
                if browser:
                    await browser.close()
                    await asyncio.sleep(1)
            except:
                pass


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
def main():
    """MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    mcp.run(transport="stdio")

if __name__ == "__main__":
    main()
